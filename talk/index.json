[{"authors":["admin"],"categories":null,"content":"Richard is a Lead Developer for Katana at Foundry, and previously head of the National Centre for Computer Animation at Bournemouth University. He has undertaken research in many areas of Computer Graphics, including dynamical modelling, fluid simulation, geometry processing, virtual reality and real-time rendering. In collaboration with partners in the Visual Effects industry he continues to develop high performance and practical solutions to many problems related to film production.\n","date":1561420800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1561420800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rsouthern.github.io/author/richard-southern/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/richard-southern/","section":"authors","summary":"Richard is a Lead Developer for Katana at Foundry, and previously head of the National Centre for Computer Animation at Bournemouth University. He has undertaken research in many areas of Computer Graphics, including dynamical modelling, fluid simulation, geometry processing, virtual reality and real-time rendering.","tags":null,"title":"Richard Southern","type":"authors"},{"authors":null,"categories":null,"content":"A talk (now a bit old) talking about our work on using Smoothed Particle Hydrodynamics for Blue Noise sampling. I\u0026rsquo;ll talk about the advantages and disadvantages of using this method, along with applications and future research directions. There is also an introduction to SPH in there too.\n The slides for this talk can be viewed here.\n","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"a53bf26881c023ea7947654c38d19068","permalink":"https://rsouthern.github.io/talk/sphsampling/","publishdate":"2020-10-08T00:00:00Z","relpermalink":"/talk/sphsampling/","section":"talk","summary":"A talk about our work on using Smoothed Particle Hydrodynamics for Blue Noise sampling.","tags":["presentation","sph","simulation"],"title":"SPH Sampling and its applications in Computer Graphics","type":"talk"},{"authors":["Richard Southern","richard jones"],"categories":null,"content":"Media Access talk slides and accompanying notes here.\n","date":1561420800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561420800,"objectID":"ce6f8204c021e0d5583b42cb4876cdd8","permalink":"https://rsouthern.github.io/publication/pic/","publishdate":"2019-06-25T00:00:00Z","relpermalink":"/publication/pic/","section":"publication","summary":"In this talk I discuss the use of the Particle In Cell method and it's  use in contemporary Visual Effects.","tags":["fluid","simulation"],"title":"On the use of Particle-in-Cell methods in Visual Effects","type":"publication"},{"authors":null,"categories":null,"content":"In this post I will discuss the math and code behind a method to generate the automated orientation of a shading vector field using the intrinsic geometric properties of the object.\nBackground Anisotropic reflectance models are handy for modeling shading behaviours at the microfacet level which conform with some sort of oriented field, for example, brushed metal: The Ward anisotropic specular reflectance model can (after some simplification, for which I unfortunately do not have a clear reference) be stated as: \\begin{equation} \\frac{(\\mathbf{l}\\cdot \\mathbf{n})}{(\\mathbf{v}\\cdot \\mathbf{n})} \\exp \\left( -2 \\frac{ \\left(\\frac{\\mathbf{h}\\cdot\\mathbf{x}}{\\alpha_x} \\right)^2 + \\left(\\frac{\\mathbf{h}\\cdot \\mathbf{y}}{\\alpha_y} \\right)^2}{(1+\\mathbf{h} \\cdot \\mathbf{n})}\\right), \\end{equation} where \\(\\mathbf{n}\\) is the surface normal at a point, $\\mathbf{l}$ is the vector from the point to the light source, \\(\\mathbf{v}\\) is the vector from the point to the eye and \\(\\mathbf{h}\\) is the half vector defined as \\(\\mathbf{h}=(\\mathbf{l}+\\mathbf{v})/\\|\\cdot \\| \\). The important anisotropic bits are the vectors \\(\\mathbf{x}, \\mathbf{y}\\), which define the (orthogonal) directions of anisotropy, and \\(\\alpha_x\\) and \\(\\alpha_y\\) define the standard deviation of the surface slope in the \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) directions respectively. They scale the \u0026ldquo;amount\u0026rdquo; of anisotropy in each of the directions. Note that \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), and \\(\\mathbf{n}\\), should define an orthogonal coordinate frame.\nThis might be implemented in a fragment shader as something similar to this:\nvec3 halfwayVector = normalize(lightDirection + viewDirection); vec3 normalDirection = normalize(FragNormal); vec3 tangentDirection = normalize(FragK1); vec3 binormalDirection = normalize(FragK2); // this also works: cross(normalDirection, tangentDirection); float dotLN = dot(lightDirection, normalDirection); vec3 diffuseReflection = attenuation * Light.Ld * Material.Kd * max(0.0, dotLN); float dotHN = dot(halfwayVector, normalDirection); float dotVN = dot(viewDirection, normalDirection); float dotHTAlphaX = dot(halfwayVector, tangentDirection) / alphaX; float dotHBAlphaY = dot(halfwayVector, binormalDirection) / alphaY; vec3 specularReflection; if (dotLN \u0026lt; 0.0) // light source on the wrong side? {specularReflection = vec3(0.0);} else { // light source on the right side specularReflection = attenuation * Material.Ks * max(0.0,(dotLN/dotVN)) * exp(-2.0 * (dotHTAlphaX * dotHTAlphaX + dotHBAlphaY * dotHBAlphaY) / (1.0 + dotHN)); } fragColor = vec4(specularReflection + diffuseReflection, 1.0);  In order for us to generate good quality anisotropic fields using this formulation we see that we would have to provide per-pixel values for \\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\alpha_x\\) and \\(\\alpha_y\\) (note that as \\(\\mathbf{y}\\) is orthogonal to \\(\\mathbf{x}\\) and \\(\\mathbf{n}\\) so theoretically this can be deduced). Typically in a shader the vector field would be inferred by interpolation from a texture map containing direction vectors. As this involves a lot of effort, I looked into something a little more automated.\nLines of Principal Curvature Most interesting surfaces are curved in some way. You\u0026rsquo;re probably already familiar with the concept of a normal to a surface: if we zoom in enough to a surface (or manifold) it will look pretty flat. The vector defining the orientation of this plane is called a normal at a point on the surface.\nInterestingly there are two other vectors that are defined in that tangent plane - one defines the direction and magnitude in which the rate the normal changes across the surface is at a maximum, and other the least - these are called the maximum and minimum principal curvature directions respectively, and are often symbolized by \\(\\mathbf{k}_1\\) and \\(\\mathbf{k}_2\\) respectively.\n The figure above demonstrates the principal curvature directions for a saddle point, which lie in the orthogonal bisecting planes which intersect at the point where the curvature is measured. Interestingly a geometrically perfect saddle point and sphere would have $ \\\\|\\mathbf{k}_1\\\\| = \\\\|\\mathbf{k}_2\\\\| $. These vectors are defined at all points over a smooth surface, and seem like a good choice for an intrinsic surface property suitable for defining our shading orientation, i.e. let \\(\\mathbf{k}_1 = \\alpha_x \\mathbf{x}\\) and \\(\\mathbf{k}_2 = \\alpha_y \\mathbf{y}\\). However there is a snag - the flow of the principal curvature direction is undefined, e.g. \\(\\mathbf{k}_1, \\mathbf{k}_2\\) and \\(-\\mathbf{k}_1, -\\mathbf{k}_2\\) are both equally valid principle curvature directions at any point as the frame can be arbitrarily flipped around (which we\u0026rsquo;ll see later is a bit of a problem).\nComputing the Principal Curvature Directions There are a number of methods to compute principal curvature directions over a triangle mesh. I\u0026rsquo;m not going to dwell on it here. The library I used for this is libigl which provides a simple header only library based on the math library Eigen. The full explanation of how libigl deduces the principle curvature directions is described here.\nThe code for loading up a mesh, deducing vertex normals and extracting the principal curvature directions is below:\n// Read a mesh from a file into igl MatrixXfr V; MatrixXir F; igl::read_triangle_mesh(filename, V, F); // Determine the smooth corner normals MatrixXfr N; igl::per_vertex_normals(V,F,N); // Compute the principle curvature directions and magnitude using quadric fitting MatrixXfr K1,K2; Eigen::VectorXf KV1,KV2; igl::principal_curvature(V,F,K1,K2,KV1,KV2);  Note that the KV1 and KV2 hold the magnitude of each vector (K1 and K2 are normalized). This seemed a bit wasteful so I recombined these into a single vertex structure which contained the position, normal and scaled \\(\\mathbf{k}_1\\) and \\(\\mathbf{k}_2\\):\nMatrixXfr KV1_mat(K1.rows(), K1.cols()); KV1_mat \u0026lt;\u0026lt; KV1,KV1,KV1; MatrixXfr KV2_mat(K2.rows(), K2.cols()); KV2_mat \u0026lt;\u0026lt; KV2,KV2,KV2; K1.array() = KV1_mat.array() * K1.array(); K2.array() = KV2_mat.array() * K2.array(); // Now concatenate our per vertex data into a big chunk of data in Eigen MatrixXfr Vertices(V.rows(), V.cols() + N.cols() + K1.cols() + K2.cols()); Vertices \u0026lt;\u0026lt; V, N, K1, K2;  Be wary as what these libraries gain in ease of use they sacrifice in terms of performance, so the application takes quite a while on startup to compute these matrices and vectors.\nThe curvature vectors generated using this process can be visualised as follows: In this case the coordinate frame at each point is defined by the normal (in red), \\(\\mathbf{k}_1\\), the maximum curvature direction and magnitude (in blue) and \\(\\mathbf{k}_2\\), the minimum curvature direction (in green).\nEarly results This all sounds like a great idea, but when we render these vectors, we get something a bit unexpected: Actually, these results are entirely predictable. For a first implementation, the curvatures are defined as vertex properties, which are simply passed on to the fragment shader to shade. This means that the curvatures are being intepolated across the face per fragment.\nLooking at the curvature directions from the previous section, we can see that the curvature vectors do not define a consistent flow over the surface. This is because the curvature is deduced independently at each vertex, and perfectly valid in either direction of flow, e.g. I could rotate by 180 degrees about the normal and the principle curvature directions are the same.\nTo the Ward shading algorithm it doesn\u0026rsquo;t matter, as \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are effectively squared so the sign will disappear. However, during rasterization the vectors will be interpolated across the face - the curvature vectors at the corners of a face may be pointing in opposite directions, which means that the interpolated vector will sweep between them.\nOne option is to re-orient all of the curvature vectors as a preprocess. The only method I\u0026rsquo;m aware of to consistently orient the curvature field is based on the method described in Anisotropic Polygonal Remeshing by Alliez et al. (also available [here]) but this involves smoothing the tensor field, identifying umbilics and iteratively growing streamlines across the surface, which seemed somewhat tedious.\nCurvature Direction Correction on the GPU One of the convenient properties of the rendering process is that the triangle mesh (which may have shared vertex properties) is essentially broken up into independent triangles for the purposes of rendering as fragments. This effectively means that the vertex properties will be duplicated for each corner of each face coincident on a vertex.\nEach face with the vertex properties to be interpolated are passed on to a Geometry Shader before the Fragment Shading stage of the pipeline. At this stage we are free to flip around \\(\\mathbf{k}_1\\) and \\(\\mathbf{k}_2\\) in order to ensure that all the curvature vectors at the corners are consistently oriented for this particular face. Note that which a neighbouring face might have these vectors oriented in a different direction, it doesn\u0026rsquo;t matter due to the properties of the Ward shading model.\nvoid main() { // The first triangle vertex is just going to be copied across gl_Position = gl_in[0].gl_Position; FragPosition = GeoPosition[0]; FragNormal = GeoNormal[0]; FragK1 = GeoK1[0]; FragK2 = GeoK2[0]; EmitVertex(); for(int i = 1; i \u0026lt; 3; ++i) { // Copy across the vertex position gl_Position = gl_in[i].gl_Position; FragPosition = GeoPosition[i]; FragNormal = GeoNormal[i]; // Determine whether the curvature is flipped, if so, correct the curvature normal FragK1 = ((dot(GeoK1[0], GeoK1[i]) \u0026lt; 0.0)?-1.0:1.0) * GeoK1[i]; FragK2 = ((dot(GeoK2[0], GeoK2[i]) \u0026lt; 0.0)?-1.0:1.0) * GeoK2[i]; // Emit the vertex of this primitive EmitVertex(); } // Finish the triangle (strip) EndPrimitive(); }  As you can see all this does is copy across the first vertex, and then copy across the other two triangle vertices, but first possibly flipping the curvature vectors if they are oriented differently to the first vertex. Not particularly high-brow stuff, but it gets the job done.\nFinal Results Here is the bust model with three different scalings of the anisotropy aligned to the principle curvature directions:\n       While the face flipping problem has been fixed, the obvious issue with these results in that the curvature is not particularly smooth, mainly because of the resolution of the model but also because there has been not smoothing of the vector field. There are also issues relating to the placement of umbilics. These things could be improved by performing smoothing as described in this method or using a mesh with a higher resolution. In addition, a method which allows the user to \u0026ldquo;hide\u0026rdquo; umbilic points in geometric areas where they won\u0026rsquo;t be seen would be useful.\nDownloads The source code is hosted on GitHub. Clone it from the repository using the following command from your console:\ngit clone https://github.com/rsouthern/Examples  This example is under rendering/curv.\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3acedb739a2e220bd6750346fe93932f","permalink":"https://rsouthern.github.io/post/curv/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/post/curv/","section":"post","summary":"Automated orientation of the shading vector field using the intrinsic geometric properties of the object.","tags":["opengl","glsl","rendering"],"title":"Principal Curvature Aligned Anisotropic Shading","type":"post"},{"authors":null,"categories":null,"content":"Background Cel shading is a common shading technique designed to make 3D geometry look like it was hand painted in a traditional way. While it is fairly trivial to darken vertices on the silhouette, to truly emulate the effect of a hand drawn outline, the ink outline must extend outside the silhouette of the object at a constant thickness for it to appear plausible. Games like Borderlands make liberal use of outline geometry to enhance the non-photorealistic style of the game.\nMy starting point was the blog post by Philip Rideout in which gaps between fins are blended using a blurring pass, which seemed wasteful, and may prohibit procedural outline effects like glowing or outline particles.\nI started from scratch, but what I ended up with is a method similar to Single Pass GPU Stylized Edges by Hermosilla and Vazquez, although implemented on the geometry shader, and also requiring about half the additional geometry. I would also argue that the method I\u0026rsquo;ve produced here is considerably simpler than any previous method.\nWhat is the Silhouette? This is probably a good place to start. Assume for the moment that we have some nice continuous object (not a triangle mesh) like a perfect sphere. The silhouette is effectively a connected curve around the object where the surface of the object is exactly orthogonal to the direction of the viewer. Consider the horizon - the silhouette is the exact point at which the ground meets the sky.\nBased on this we could formulate a simple definition of the silhouette as the set of all points \\(x\\) for which \\(\\mathbf{v}_x \\cdot \\mathbf{n}_x = 0 \\), where \\(\\mathbf{v}_x\\) is the view vector from the eye to \\(x\\) and \\(\\mathbf{n}_x\\) is the normal at the point \\(x\\).\nMethod Overview Determining the silhouette of a triangle mesh Now lets consider this on a triangle mesh. Any silhouette is only going exist between points on two edges of a subset of triangles on the mesh. We will assume that our triangle mesh is sufficiently dense so we can approximate the silhouette across the triangle face with a straight line (it would be possible to generate smoother curves using a Bezier spline or equivalent). The comparison of the discrete and the continuous silhouette case is demonstrated in the figure below:\n Remember that a triangle mesh is typically an approximation of some smoother surface representation, and should in most cases have vertex normals defined which effectively allow us to generate smooth shading across each face, giving the illusion of a continuous surface representation. We can use this knowledge to determine which faces are on the silhouette.\nConsider an edge with vertices at \\(p_{0,1} \\) with vertex normals \\( \\mathbf{n}_{0,1} \\) respectively. If \\(\\mathbf{v}_0 \\cdot \\mathbf{n}_0 \u0026lt; 0 \\) and \\(\\mathbf{v}_1 \\cdot \\mathbf{n}_1 \u0026gt; 0 \\) (or visa versa), then somewhere along this edge there is an \\(x\\) for which \\(\\mathbf{v}_x \\cdot \\mathbf{n}_x = 0 \\).\nA wary reader will note that \\(\\mathbf{v}_0 \\neq \\mathbf{v}_1\\) which might cause problems, but as we will see, all view vectors \\(\\mathbf{v}_x\\) will be the same \\(\\forall x\\).\nInterpolation So we need to find the point \\(x\\) across the edge for which \\(\\mathbf{v}_x \\cdot \\mathbf{n}_x = 0 \\). We\u0026rsquo;re going to make a couple of assumptions to simplify things. The first assumption is that we\u0026rsquo;re going to use simple linear interpolation for performance reasons. Accuracy will be affected, but we\u0026rsquo;re going to assume that the input mesh is dense and any accuracy issues will not be noticeable.\nLinear interpolation would imply that we\u0026rsquo;re looking for some \\(t\\) for which \\((1-t)\\mathbf{v}_0 \\cdot \\mathbf{n}_0 + t \\mathbf{v}_1 \\cdot \\mathbf{n}_1 = \\mathbf{v}_x \\cdot \\mathbf{n}_x = 0 \\), e.g. the interpolation parameter for which the the normal is zero.\nThe second assumption we\u0026rsquo;ll be creating the geometry of the fin on the geometry shader, which means that by convention we can rely on the fact that the geometry has already been projected, which means it already lives within a canonical viewing volume. We can now reliably state that \\(\\mathbf{v}_0 = \\mathbf{v}_1 = [0,0,-1]^T\\), i.e. the view vector for all vertices is just from the origin looking in the \\(-z\\) direction by the OpenGL camera convention.\nSo this means that we can simplify the formulation significantly to just \\( (1-t) \\mathbf{n}_{(0,z)} + t\\mathbf{n}_{(1,z)} = 0 \\), which yields \\( t = -\\mathbf{n}_{(0,z)} / (\\mathbf{n}_{(1,z)}-\\mathbf{n}_{(0,z)}) \\). Built into this formula is also the implicit test: \\( 0 \\leq t \\leq 1 \\implies \\) the edge crosses the silhouette.\nBelow is this trivial function as implemented in the geometry shader:\n/** This is where most of the magic happens - it determines the interpolation value * based on the z values of the two normals in order to detemrine a normal pointing * out orthogonal to the view direction. * \\param n0,n1 The two input normals * \\return The output interpolation value (between 0 and 1 if edge is on the silhouette) */ float isSilhouette(in vec3 n0, in vec3 n1) { // Trivial case: the normals are the same, so we'll just pick a point in the middle if (n1.z == n0.z) { return 0.5; } else { // Use our formula to determine the interpolation value and return a boolean based // on whether the interpolant is within the two input vectors return - n0.z / (n1.z - n0.z); } }  Putting it together The geometry shader to generate fins accepts a triangle as input and spawns a triangle strip of 4 vertices as output. Each triangle is going to be processed, and we can expect neighbouring triangles on the silhouette to be consistent - see the image below for an example (I appreciate that it\u0026rsquo;s not great):\n The geometry shader iterates over all edges of the triangle and determines the value of \\(t\\) to see if it lies on the silhouette. If it does, a fin vertex and normal is determined using a straight linear interpolation (via the built-in mix function):\n// Iterate over all the edges in our triangle int j; for (int i = 0; i \u0026lt; 3; ++i) { j = (i+1)%3; t[cnt] = isSilhouette(normal[i], normal[j]); if ((t[cnt] \u0026gt;= 0.0) \u0026amp;\u0026amp; (t[cnt] \u0026lt;= 1.0)) { finVerts[cnt] = mix(pos[i], pos[j], t[cnt]); finNorms[cnt] = normalize(mix(normal[i], normal[j], t[cnt])); ++cnt; } }  I did also experiment with higher order mixing methods, but these showed no discernable improvement and just added significantly to the computation cost and complexity so these approaches were scrapped.\nNow if the variable cnt is exactly 2 then we know that this triangle cuts the silhouette and we need to generate a triangle strip fin:\n// If count is less than 2 don't do anything as there isn't a fin if (cnt \u0026gt;= 2) { // Create our triangle strip from the two input vertices and normals finColour = vec4(0,0,0,1); // You might want to visualise something with the color gl_Position = vec4(finVerts[0],1.0); EmitVertex(); gl_Position = vec4(finVerts[1],1.0); EmitVertex(); gl_Position = vec4(finVerts[0] + finScale * finNorms[0],1.0); EmitVertex(); gl_Position = vec4(finVerts[1] + finScale * finNorms[1],1.0); EmitVertex(); EndPrimitive(); }  Note that the rendering of the fin is an additional render pass performed after you\u0026rsquo;ve rendered the main geometry, as the geometry shader will effectively \u0026ldquo;eat\u0026rdquo; the original geometry.\nResults Here are a couple of Buddha\u0026rsquo;s of various levels of thickness:\n      The results are close to perfect: the fins are consistent about both internal and external silhouettes. There are still a couple of problems which need resolving:\n The normals are not transformed correctly after projection. This is because the normals are needed in the world coordinates for the lighting calculations. You can see the error in that the line is not the same width everywhere on the silhouette. Unfortunately I\u0026rsquo;ve not yet figured out the correct method to transform the normals to be correct according to the canonical viewing volume - feel free to contact me if you have a solution to this. The lines don\u0026rsquo;t taper when nearing the edge of an internal silhouette (leading to \u0026ldquo;sharpy edges\u0026rdquo;). This could be fixed by tiling a round texture map on the silhouette so the borders become rounded. I have not explored all the fun effects that fins can do, like glow or illustrative visualisation: this is up to you!  Downloads The source code is hosted on GitHub. Clone it from the repository using the following command from your console:\ngit clone https://github.com/rsouthern/Examples  This example is under rendering/fins.\n","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"a8377bcd991541aa2a90925ba626a8c3","permalink":"https://rsouthern.github.io/post/fins/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/post/fins/","section":"post","summary":"Generating consistent outline geometry on the geometry shader.","tags":["rendering","opengl","shaders"],"title":"Realtime Silhouette Rendering with Consistent Geometric Fins","type":"post"},{"authors":["min jiang","Richard Southern","jian j zhang"],"categories":null,"content":"Paper A preprint of the paper is available [here] (under embargo until 12/01/2019).\nMedia    ","date":1515715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515715200,"objectID":"b51191d67a75a90eb50608063e8398da","permalink":"https://rsouthern.github.io/publication/dissolution2/","publishdate":"2018-01-12T00:00:00Z","relpermalink":"/publication/dissolution2/","section":"publication","summary":"The effect of sampling on the simulation of dissolution and erosion","tags":["simulation","fluid","sph"],"title":"Energy-based dissolution simulation using SPH sampling","type":"publication"},{"authors":["richard jones","Richard Southern"],"categories":null,"content":"Paper Paper available on institutional repository [HERE]\nMedia    ","date":1501200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501200000,"objectID":"83a40ee76dc7bb63a3f1292507e4114a","permalink":"https://rsouthern.github.io/publication/droplets/","publishdate":"2017-07-28T00:00:00Z","relpermalink":"/publication/droplets/","section":"publication","summary":"In this paper we present a physically-based model for simulating realistic interactions between liquid droplets in an efficient manner.","tags":["simulation","fluid"],"title":"Physically-based Droplet Interaction","type":"publication"},{"authors":null,"categories":null,"content":"In this talk I\u0026rsquo;ll present the mathematical tools for modeling motion adaptation.\n   The slides from the talk can be viewed [here]. ","date":1501027200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501027200,"objectID":"9f83e77dd6f9997f581dcdda5807b50a","permalink":"https://rsouthern.github.io/talk/motionadaptation/","publishdate":"2017-07-26T00:00:00Z","relpermalink":"/talk/motionadaptation/","section":"talk","summary":"In this talk I'll present the mathematical tools for modeling motion adaptation.","tags":["presentation","dynamical systems","robotics"],"title":"Modelling Adaptive Human Locomotion - Lessons from Motor Control, Robotics and Biomechanics","type":"talk"},{"authors":null,"categories":null,"content":"Introduction The icon for this page was from my attempt to recreate a realtime shader for a varnished woodgrain effect. This approach requires the combination of multiple layers of Perlin noise, which is probably infeasible to generate on the fly within the fragment shader (although I\u0026rsquo;d love to be proven wrong). The generation of the wood grain shader will be saved for another post - in this one I\u0026rsquo;d like to discuss how I implemented a generic C++ template for the initialisation of the \u0026ldquo;general\u0026rdquo; dimensional noise texture on OpenGL.\nThe High Level Interface The central idea behind this class was to provide the following generalisations:\n General Dimensional Textures - noise textures are typically either 2D or 3D (so you can \u0026ldquo;slice through\u0026rdquo; your material and expect a consistent pattern). Generic Generator Functions - the function that actually generates the noise must be completely generic.  For (1) above, this is realised by using a simple template parameter DIM:\ntemplate \u0026lt;size_t DIM\u0026gt; class NoiseTexture  So my interface to creating a 3D texture is something like:\nMyNoiseTexture\u0026lt;3\u0026gt; m_noise;  For (2), there were several ways to approach this. I could, for example, have passed a generator function / functor class as a template parameter. This would have required me to create different classes for each type of generation. However there are a couple of reasons why I used inheritance instead of this approach (which could be called the strategy pattern):\n A function cannot store state, which is need for many noise generators, for example boost or libnoise, A functor can store state, but will have the same level of class management and complexity as using inheritance, but more importantly I may want to override other parts of the class to support parallel noise generation.  The interface noisetexture.h provides is to inherit from the base class and override the protected function\n/// A generator function to make the process of building noise easy virtual inline GLfloat generator_func(const CoordinateArrayf \u0026amp;) = 0;\t This function simply accepts a coordinate in DIM space and returns a floating point value. Note that the NoiseTexture makes use of std::array as it should - this makes sure that this is managed memory rather than pointers. Two different arrays are defined:\n/// A static (i.e. not dynamic allocated) vector typedef std::array\u0026lt;size_t,DIM\u0026gt; CoordinateArray; typedef std::array\u0026lt;float, DIM\u0026gt; CoordinateArrayf;  One is for indices and the other for positions. This will be used in the recursive generation of the data block.\nGenerating the data Ultimately the initialisation of the OpenGL texture consists of copying a lump of data from the CPU memory to the GPU memory using a call to glTexImage. The following section of code creates the memory, fills it with data and copies it to the GPU:\n// Allocate a slab of data for the stuffing GLfloat *data = (GLfloat*) malloc(sizeof(GLfloat) * pow(m_res,DIM) * 3); // Use the recursive function to generate the data recursively CoordinateArray coord; generate_recurse(DIM, coord, data); // Copy our data over to the GPU copyTextureDataToGPU(data); // Delete our data - it's been copied onto the GPU right? free(data);  Unpicking the memory allocation from left to right:\n sizeof(GLfloat) - the size of an individual value (single precision) pow(m_res,DIM) - the size of the block of data is squared/cubed as you would expect 3 - we need RGB values to copy to the GPU  Generate_recurse is defined as follows:\n/// Recursively generate the data virtual void generate_recurse(const size_t \u0026amp;/*dim*/, const CoordinateArray \u0026amp;/*coord*/, GLfloat */*data*/);  It will recursively evaluate the generator function on each of the different positions in the block of memory and write it to the appropriate index:\ntemplate \u0026lt;size_t DIM\u0026gt; void NoiseTexture\u0026lt;DIM\u0026gt;::generate_recurse(const size_t \u0026amp;d, const CoordinateArray\u0026amp; coord, GLfloat *data) { size_t dim_length = 1, data_pos = 0, i; CoordinateArrayf coordf, tmp; CoordinateArray ncoord; switch(d) { case 0: // At the end of the recursion chain we can evaluate the noise function for (i=0; i\u0026lt;DIM; ++i) { data_pos += coord[i] * dim_length; dim_length *= m_res; coordf[i] = m_inv_resf * float(coord[i]); } data_pos *= 3; // Fill up the data with data defined by the generator_func() for (i=0; i\u0026lt;3; ++i) { tmp = coordf; coordf[i] += 1.0f; data[data_pos + i] = generator_func(coordf); } break; default: // For the rest of the dimensions we recursively call all the remaining coordinates ncoord = coord; for (i=0; i\u0026lt;m_res; ++i) { ncoord[d-1] = i; generate_recurse(d-1, ncoord, data); } } }  Note that I have arbitrarily added 1.0f for each of the R, G, B channels to give a bit of a richer data set to play with.\nOpenGL integration There are a number of standard functions which make this class usable in a GL context. For example, the function to find the texture:\ntemplate \u0026lt;size_t DIM\u0026gt; void NoiseTexture\u0026lt;DIM\u0026gt;::bind() const { if (m_isInit) { glBindTexture(m_target, m_texID); CheckError(\u0026quot;NoiseTexture\u0026lt;DIM\u0026gt;::bind() - glBindTexture()\u0026quot;); } }  Note the variable m_target, which is actually a constant, defined using this from within the class declaration:\nconstexpr GLuint target() const { switch(DIM) { case 1: return GL_TEXTURE_1D; case 2: return GL_TEXTURE_2D; case 3: return GL_TEXTURE_3D; } return 0; } const GLuint m_target = target();  Take a moment to digest this: the function target is actually being executed at compile time, and this is in fact an example of template metaprogramming (albeit, a rather simple one). The keyword constexpr allows this function to be executed in setting a constant at compile time. The use of the switch statement (and using multiple return statements in a function) is only allowable in C++14 or better (a simple if-else structure would remove this dependency).\nThe function which copies the data to the GPU is pretty straightforward:\ntemplate \u0026lt;size_t DIM\u0026gt; void NoiseTexture\u0026lt;DIM\u0026gt;::copyTextureDataToGPU(GLfloat *data) { // Transfer this data to our texture glGenTextures(1, \u0026amp;m_texID); glBindTexture(m_target, m_texID); // Repeat the texture for coordinates \u0026lt;0 or \u0026gt;1 ... // Use blending when texels are bigger or smaller than pixels ... // Upload the texture data to the GPU switch(DIM) { case 1: glTexImage1D(m_target, // Target 0, // Level GL_RGB, // Internal Format m_res, // width 0, // border GL_RGB, // format GL_FLOAT, // type data); break; case 2: ... case 3: ... } }  Note that template specialisms could have been used here for the different TexImage calls, but due to the duplication of the texture parameters I decided against it.\nDeriving your own Noise Texture class The simplest example of a derived noise texture class is one which just sets the values in the data block to 1. This is demonstrated in testnoisetexture.h:\ntemplate\u0026lt;size_t DIM\u0026gt; class TestNoiseTexture : public NoiseTexture\u0026lt;DIM\u0026gt; { public: /// Constructor explicit TestNoiseTexture(float /*lower*/ = 0.0f, float /*upper*/ = 1.0f, size_t /*resolution*/ = 64); /// Dtor ~TestNoiseTexture() {} protected: /// Specialisation of this class to generate pure white noise inline GLfloat generator_func(const typename NoiseTexture\u0026lt;DIM\u0026gt;::CoordinateArrayf \u0026amp;); }; template\u0026lt;size_t DIM\u0026gt; TestNoiseTexture\u0026lt;DIM\u0026gt;::TestNoiseTexture(float _lower, float _upper, size_t _resolution) : NoiseTexture\u0026lt;DIM\u0026gt;(_lower,_upper,_resolution) {} template\u0026lt;size_t DIM\u0026gt; inline GLfloat TestNoiseTexture\u0026lt;DIM\u0026gt;::generator_func(const typename NoiseTexture\u0026lt;DIM\u0026gt;::CoordinateArrayf \u0026amp;coord) { return 1.0f; }  As you see above, all you need to do here is override the virtual generator_func with your own favourite noise generator. See the whitenoisetexture.h example for something a little more interesting. I\u0026rsquo;ll be looking to write a post about how this method was used to create the wood shader soon.\nDownloads Download the source files for these here:\n noisetexture.h testnoisetexture.h whitenoisetexture.h  ","date":1467244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467244800,"objectID":"53ee59c39e4cda1badc8588208a54ed8","permalink":"https://rsouthern.github.io/post/noisetexture/","publishdate":"2016-06-30T00:00:00Z","relpermalink":"/post/noisetexture/","section":"post","summary":"An implementation of an extensible template class in C++ for different types of OpenGL noise. Uses some interesting features other than templates, like constexpr, recursion etc.","tags":["rendering","noise","shaders","opengl","c++"],"title":"A Templated Implementation of Noise Texture","type":"post"},{"authors":null,"categories":null,"content":"Background Continuing with the theme of nD implementations of useful algorithms, here is my take on Moving Least Squares (MLS), a popular method used in engineering and graphics applications for smoothing out noisy point cloud data. Possibly data sources include laser scans, depth sensors (e.g. Kinect) or the like.\nThe basic principle of MLS projection is that you have a noisy point cloud approximating a surface. Projection means we\u0026rsquo;re going to take a point from anywhere in space and project it on this surface approximation. We will conveniently also get a surface normal for free as a result of doing this.\nThe Basic Algorithm I\u0026rsquo;ll describe the projection algorithm with reference to the following figures:\n      We are given the original query site \\\\(q_0)\\in\\mathbb{R}^n\\\\) (the red point) and the existing point cloud consisting of points \\\\(x_i \\in \\mathbb{R}^n\\\\) (the black points) (note I'm going to use the indices of the query site and the point cloud differently - the index of \\\\(q\\\\) refers to the iteration number of the algorith, the index of \\\\(x\\\\) will refer to index of the nearby points). We are also given some query radius \\\\(r\\\\) which will contain all the points of interest. Note that the choice of \\\\(r\\\\) has a significant effect on the smoothness of the function - this will probably be a topic of discussion for a later post. In the images from right to left: 1. Identify all the points within the query radius \\\\(r\\\\) of \\\\(q_0\\\\), i.e. the set of points \\\\(\\\\{x_i : \\\\|x_i - q_0\\\\| This is pretty trivially demonstrated using a code snippet from the implementation. In this example cpt is the current point (e.g. the iterating projected point \\(q_j\\)), tol is some user specified tolerance (equivalent to \\(\\epsilon\\)). The rest should mostly be self-explanatory:\n// Continue this loop until we're close enough to the surface while (err \u0026gt; tol) { // Perform a least squares projection from the current point onto a best // fit hyperplane. The result will tell us if any neighbours were found. if (weightedLS(cpt, radius, plane, idxDist) == 0) { // What should we do here? throw an error? return cpt; } // If it all went well, we'll have a plane onto which the point can be projected tmp = plane.projection(cpt); // Our error is just the distance between the input point and our current point err = (tmp-cpt).norm(); // Update our current point to the projected point cpt = tmp; } // If it all went well, we can assume our current point is the best one available return cpt;  Note that the function weightedLS() returns the number of points found within the query radius, and modifies the plane to contain the best fit plan through the points. If nothing was found within the radius there is a question of what you should do. Increasing the radius size is an obvious choice, but there may be continuity issues in the query data set.\nWeighted Least Squares So it is now important to drill down into the Weighted Least Squares fit function. There are some pretty useful references on everyones favourite academic source here. There is a much broader discussion here to be had about linear regression for fitting curves to polynomials, but I\u0026rsquo;ll restrict myself to the planar case.\nThe general implicit form for a plane $$\\mathbf{n}\\cdot(x - x_0)=0,$$ where \\(\\mathbf{n}\\) is a normal to the plane (bold because it is a vector), and \\(x_0\\) is a point through which the plane passes. So the plane is defined by any point \\(x\\) for which the above function evaluates to exactly \\(0\\).\nIf the number of points is equal to the dimensionality the space it is easy enough to find the normal from the input points. For example, the normal to a vector $$ \\mathbf{v}= \\left[ \\begin{array}\\ x_1 - x_0 \\\\ y_1 - y_0 \\end{array} \\right] $$ between two points in \\(\\mathbb{R}^2\\) is \\((-v_y,v_x)\\), for higher dimensions use the cross or wedge product.\n As an aside, you might be interested to know that this can also be solved by solving for the null space of the system of equations given by \\(\\mathbf{n}\\cdot(x - x_0)=0\\) - recall that this system is underdetermined because one of the points provided will be \\(x_0\\). In \\(\\mathbb{R}^2\\), this will be the equivalent of solving for the null space of $$ \\begin{array}\\\n\\mathbf{n} \\cdot \\mathbf{v} = 0\\\nn_x v_x + n_y v_y = 0 \\end{array} $$ which is of course only non-trivially true when \\(\\mathbf{n} = (-\\mathbf{v}_y,\\mathbf{v}_x)\\).\n Note that we can also solve for the normal using the formula \\(\\mathbf{n}\\cdot(x - x_0)=\\alpha\\) where \\(\\alpha\\) is some arbitrary scalar as the normal is the same if we shifted it along the normal direction. This allows us to formulate the problem into a linear system $$X\\textbf{n}=\\textbf{b},$$ where \\(X\\) is the matrix of known points on the plane, \\(\\mathbf{n}\\) is the unknown plane normal, and \\(\\mathbf{b}\\) is some constant vector made up of \\(\\alpha\\)\u0026lsquo;s. For simplicity let\u0026rsquo;s just let \\(\\alpha=1\\) from now on. This is well defined if \\(X\\) is square, i.e. there are the same number of points as there are dimensions - this is because these points form a n-simplex in \\(\\mathbb{R}^n\\).\nOk, now what happens if we have more points than dimensions? In this case, we can solve this in the least squares sense. This is essentially an optimisation problem, solved using the Moore-Penrose pseudoinverse. While there is buckets to write about this, it boils down to premultiplying both sides of the equation by \\(X^T\\) yielding $$X^T X \\mathbf{n} = X^T \\mathbf{b}.$$ Note that \\(X^T X\\) is now a square matrix, the system can be solved and has a unique solution. The solution itself is optimal in a least-squares sense, meaning that it minimizes the sum of the squares of the errors made in the results of each equation. Or put another way, it spreads the error love evenly between all of the point contributors to the plane.\nNow applying weights to this should actually be quite straightforward: we\u0026rsquo;ll create some diagonal matrix of weights \\(W\\) where elements on the diagonal represent the amount we want to weight the error of each point in \\(X\\). So a smaller weight would mean that in a least squares sense we are consider the error to be less important. This is incorporated into the system above as follows: $$X^T W X \\mathbf{n} = X^T W \\mathbf{b}.$$ If we\u0026rsquo;re trying to write our plane equation as \\(\\mathbf{n}\\cdot x = d\\) where \\(d\\) is a constant, a property of the above process is that \\(d=1 / |\\mathbf{n}| \\). Also you must still normalise the resultant normal, e.g. \\(\\mathbf{n} = \\mathbf{n}d\\). Note that this whole process works in any dimension - pretty neat.\nThere is also the small issue of how to decide on the weights. Any kernel function would do, and each will have an impact on the overall quality of the reconstruction. Currently I\u0026rsquo;m just inverting the distance, but this could lead to problems when the point is exactly on top of the query point. Other choices of kernel function, such as \\(\\exp({-s^2})\\). Some experimentation depending on application will probably be required.\nHere is the implementation of this in pointcloud.hpp:\nif (m_index.findNeighbors(results, pt.data(), nanoflann::SearchParams())) { // Weights matrix MatrixXr W(idxDist.size(), idxDist.size()); W.setZero(); // The matrix of the actual points MatrixXr K(idxDist.size(), DIM); // The result of the call is true if a neighbour could be found typename ResultsVector::iterator it; int i,j; for (it = idxDist.begin(), i=0; it != idxDist.end(); ++it,++i) { // First check to see if this evaluates to 0 exactly (BAD) if ((*it).second == REAL(0)) { // Set the weight to half the max REAL value (risky) W(i,i) = std::numeric_limits\u0026lt;REAL\u0026gt;::max() * REAL(0.5); } else { // Set the weight to the inverse distance W(i,i) = 1.0 / (*it).second; } // Build the matrix K out of the point data for (j=0;j\u0026lt;DIM;++j) K(i,j) = m_data[(*it).first][j]; } // Normalise the weights to make sure they sum to 1 W = W / W.sum(); // Now solve the system using the following formula from MATLAB // N' = (K'*diag(W)*K) \\ (K')*diag(W)*(-ones(size(I,1),1)); // d = 1/norm(N); // N = N' * d; VectorDr ans; MatrixXr A = K.transpose() * W * K; VectorDr b = -K.transpose() * W.diagonal(); ans = A.llt().solve(b); REAL d = REAL(1) / ans.norm(); Hyperplane _h(ans * d, d); h = _h; // For the results structure to be useful we need it to store the weights from W, so // we copy these back into it for (it = idxDist.begin(), i=0; it != idxDist.end(); ++it,++i) { (*it).second = W(i,i); } // We return the number of effective matches return idxDist.size(); } else { // No neighbours found! What do we do? return 0; }  The general dimensional hyperplane is managed by Eigen::Hyperplane which allows us to do projections (see previous code snippet). Note that the solver used is Eigen\u0026rsquo;s Cholesky solver, which is very fast by requires a matrix that is symmetric positive definite. Fortunately our matrix \\(X^T W X\\) has these properties. In practice it is very unwise to solve systems like \\(A\\textbf{x}=\\textbf{b}\\) by inverting \\(A\\).\nImplementation details The plan was to make this implementation general dimensional, and this has been achieved through liberal use of templates.\ntemplate \u0026lt;typename REAL, unsigned int DIM\u0026gt; class PointCloud {  The top of our class says that we\u0026rsquo;re allowing you to choose your type to represent the data (called REAL) and the dimension of the data set, which can be anything you like. As mentioned previously, Eigen provides general dimensional hyperplane routines which makes our lives relatively simple.\nUnfortunately as you may have realised from the above algorithm the performance is largely dependent on how quickly we can find all the points within a given radius. This is resolved using a KD tree, which essentially organises the points into a graph structure for fast searching. I\u0026rsquo;m not going to discuss the specifics of KD tree construction and querying - there are plenty of other links to help you with this. I chose to make use of nanoflann from Jose Luis Blanco-Claraco as it is a header only library making it easier to incorporate into the project build.\nOne issue which may seem non-obvious is that I have added the KD tree index object as a member of the PointCloud class:\n/// A KDTreeIndex type typedef nanoflann::KDTreeSingleIndexAdaptor\u0026lt; nanoflann::L2_Simple_Adaptor\u0026lt;REAL, PointCloud\u0026lt;REAL,DIM\u0026gt; \u0026gt; , PointCloud\u0026lt;REAL,DIM\u0026gt;, DIM\u0026gt; KDTreeIndex; /// A KDTree structure KDTreeIndex m_index;  This means that the KDTreeIndex is incorporated into the PointCloud class, although as you can see from the typedef above the PointCloud is a template parameter for the index itself. This is because the class which is passed to nanoflann needs to have a set of functions (all with the prefix kdtree_) which will be called during kd tree construction and querying, for example:\n/// Returns the dim'th component of the idx'th point in the class (random point access) inline REAL kdtree_get_pt(const size_t idx, int dim) const { return m_data[idx][dim]; }  The way this works is that we pass this object when we construct the KDTreeIndex, e.g.\n/** * Build an empty point cloud with an empty KDTree. */ template \u0026lt;typename REAL, unsigned int DIM\u0026gt; PointCloud\u0026lt;REAL,DIM\u0026gt;::PointCloud() : m_index(DIM, *this, nanoflann::KDTreeSingleIndexAdaptorParams(10)) { }  It is an interesting and rather twisting compositional arrangement, but it allows us to make sure that all the elements relevent to the point cloud in one convenient place.\nAnother interesting trick of nanoflann compatibility is the fact that I\u0026rsquo;m storing my points in Eigen, while nanoflann needs to compute distances using data stored in pointers to REAL\u0026rsquo;s. Rather than constructing a new Eigen point and copy data across, we can make use of the Eigen::Map which will just make a vector out of the existing memory rather than copying it across, hopefully saving us a couple of clocks:\n/// Returns the distance between the vector \u0026quot;p1[0:size-1]\u0026quot; and the data point with index \u0026quot;idx_p2\u0026quot; stored in the class: inline REAL kdtree_distance(const REAL *p1_data, const size_t idx_p2, size_t size) const { Eigen::Map\u0026lt;const PointType\u0026gt; p1_map(p1_data,DIM); return (m_data[idx_p2] - p1_map).norm(); }  Conclusions Currently the examples of using this code are pretty spartan as it is going to be incorporate into a significantly larger project. However, there are a couple of important things to note about this implementation:\n The choice of radius has a significant impact on the projection. In previous implementations I\u0026rsquo;ve geometrically scaled the radius until some points are found and then scaled it back after the first projection. However, this may have continuity implications. I\u0026rsquo;ve also used a precomputed smooth scalar field of radii to use for this projection to ensure that you have a constant number of neighbours - something I\u0026rsquo;ll be experimenting with in the future. There are a couple of implementation issues I still need to resolve, such as the choice of weight function and how to handle the situation when there are no neighbours in the specificed radius (see above). There is also the relationship between the radius and the cell size of the KD Tree which needs to be resolved. And some nice 3D examples! The process of using MLS projection to smooth input data (called mollification) was not discussed, but will hopefully be appended to this post at some point. The process is pretty simple: project each point in the existing set onto other points in the same set. This has some interesting performance considerations to doing it correctly (for example, the KD Tree will need to be rebuilt in each iteration). Parallelism! There are a couple of things which could be done in parallel, especially if we\u0026rsquo;re doing mollification.  Downloads The source code is hosted on GitHub. Clone it from the repository using the following command from your console:\ngit clone https://github.com/rsouthern/Examples  ","date":1459468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1459468800,"objectID":"d943a75a94df71e12070cafb29230f68","permalink":"https://rsouthern.github.io/post/mls/","publishdate":"2016-04-01T00:00:00Z","relpermalink":"/post/mls/","section":"post","summary":"An implementation of N-Dimensional Moving Least Squares on point clouds in C++.","tags":["geometry processing","parameterization","c++"],"title":"n-Dimensional Moving Least Squares projection","type":"post"},{"authors":null,"categories":null,"content":"A team of mainly 2nd year students have put together a short 45 second animation for CNNi as part of their \u0026ldquo;Visionaries 2020\u0026rdquo; project, to be broadcast worldwide from 18 March 2016, and then to appear on their website for several months. I think from the attached images you\u0026rsquo;ll agree that this is an accomplished and beautiful piece, and given a last minute change of the submission deadline by CNN they fact that they were still able to deliver on time (alongside their other coursework) is remarkable.\nYou can link directly to the animation on the CNN page [here].\nCongratulations to the team! Team JAMI:  Niya Gaydarova, Adele Olsauskaite, Chun You, Kristine Olgaard, Enno Schluender, Kyran Bishop, Paris Jones, Liz Peach and Chester Sampson  ","date":1458259200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458259200,"objectID":"61881df8899cd2285d22627253c29ac7","permalink":"https://rsouthern.github.io/post/cnn/","publishdate":"2016-03-18T00:00:00Z","relpermalink":"/post/cnn/","section":"post","summary":"A group of NCCA students were commissioned to produce a short animation for their \"Visionaries 2020\" project.","tags":["ncca"],"title":"NCCA Student Project broadcast on CNNi","type":"post"},{"authors":null,"categories":null,"content":"I\u0026rsquo;ve had a fair amount of experience in negotiating a publication out of largely negative reviews, and also a fair amount of experience in writing reviews for Journal publications. The best guidance I\u0026rsquo;ve read about the process of writing a rebuttal is here, a process which I have unwittingly duplicated over the years. I will, however, attempt to rephrase these points in terms of my personal priorities, and embellish these with examples.\nIntroduction It is important to distinguish between the reviewing process of a journal and a Conference: traditionally conference papers have a quick turnaround, and in many disciplines are reviewed purely on a short abstract rather than a full paper. Computer Graphics is an exception: authors are often enticed to attend conferences with the promise that their work will be published in a journal: for this reason full papers are almost always required. The implication of this is that the there is little flexibility - the paper must be publishable or near-publishable in it\u0026rsquo;s submitted form as there is little time to make significant changes. In my opinion, this limits the scope of the rebuttal to the acceptance of minor corrections and clarifications of misunderstandings (which will often still not result in a satisfactory outcome).\nThe type of rebuttal I\u0026rsquo;ll be writing about is one which accompanies a journal paper resubmission, which implies that you\u0026rsquo;ve already managed to get your foot in the door with a reasonably coherent submission. For journal submissions, my experience is that reviewers have the flexibility to \u0026ldquo;take a punt\u0026rdquo; on research that may be under-developed, poorly presented and simply not currently publishable - as long as there is an original idea lurking in the manuscript which could represent a significant contribution. Here is an actual reviewer result from a journal submission:\nRecommendation: Author Should Prepare A Major Revision For A Second Review Comments: As mentioned above, I find the combination of ideas in this paper interesting and useful, and I believe that eventually the method will merit publication. However, the paper is poorly written and some recent and also not so recent work is omitted and should be discussed. You can't get away with that in a SIGGRAPH submission. The reviewer has seen the potential cunningly hidden under the poor presentation, and encouraged me to resubmit for a second review. My opinion of this process is that the reviewer him/herself has actually contributed substantially to the production of this manuscript in identifying the importance and encouraging publication, which is a win for everyone. This should set the tone for your rebuttal: you must be both respectful and authoritative:\n Respect is vital if you want the reviewers to accept a 2nd draft: do not be dismissive of their ignorant or ill-informed remarks. Their discipline area may be quite distant from your own, and their misconceptions may arise from commonly held wisdom in another discipline area - in effect, you might both be correct. If you\u0026rsquo;re rude, it insults both the reviewer (who has put in the time to read and assess your work and is often a prominent researcher in the field) and the editor (who\u0026rsquo;s judgement of reviewer allocation is called into question). Authority is important in establishing your expertise in the area (which you reinforce by using evidence, see below), and also essential for reassuring the editor and reviewers alike that your contribution is going to be significant.  Below I\u0026rsquo;ll go into a little more depth of the process I use of writing a rebuttal.\nClassify reviewer comments The first and probably most vital thing you\u0026rsquo;ll need to is to actually read the review and identify in bullet points all the specific good and bad things from each reviewer. Then group together similar points made by different reviewers. As I\u0026rsquo;m ridiculously organised I\u0026rsquo;d often put this in some sort of table indicating the priority, if it is a positive or negative point, which reviewer(s) made the point, and details of the praise / criticism.\nLabel Priority +/- Reviewer Comment   C1.3 1 + R1,R3 \"The papers general motivation is interesting and I believe strong: combine a deformation model for keypoint selection. This as far as I understand has not been tried and seems to be a direction which might be proved fruitful.\"  C3.2 2 - R3 \"I am not convinced that the linear Laplacian model will produce results any different from Meyer and Anderson results in the cited paper.\"  Ranking the priority is important: you should address the concerns of the reviewers in the order of the importance. This should be in the order of *their* perceived importance rather than their own. The clue is in the text - if it says *it would be nice if* is obviously less urgent than *is essential for publication*. However, do not be fooled: you **must address every negative comment made by every reviewer**. Grouping similar comments together is going to be very important if you\u0026rsquo;re going to keep to the rebuttal page limit (which is often 5). Some flexibility is often required in grouping together similar but not exactly similar comments. Make sure you address both variants in your rebuttal.\nConstruct your argument A rebuttal, like a paper or grant proposal, must be constructed rather than simply written. This distinction is vital in the presentation of any argument - particularly if you\u0026rsquo;re in a debate and you don\u0026rsquo;t want to sound like a rambling loon. You will need to construct an argument where you deconstruct and address each negative statement. There are, I suppose, four possible ways to argue a point. In each case, your argument must be evidence based, otherwise it may simply be rejected out of hand.\n Agree and implement: you thank the reviewer for the excellent suggestion, and provide a page/line number of where your discussed / implemented this in your paper.  C6.1: Especially it is not clear how the energy scaling is written for a general system, the expression of $u_{local}$ for energy scaling and time scaling could be given in appendix. On this reviewers recommendation, we have included the expressions for $u_{local}$ in the main text body, and included a substantial appendix in which the local controllers are derived (Appendix B). We feel this is a useful addition to the paper, especially for readers not familiar with the application of Lie group symmetry to dynamic systems. Agree, but argue that it is beyond the scope of the paper. Reviewers sometimes want it all. Requests can be unreasonable for a single incremental unit of research (i.e. the journal paper). Sometimes a reality check is necessary. This is a difficult argument to make, as it requires to some extent an argument to be made that not only argues for the quality of the results in the paper to be sufficient and representative, but also that the additional work suggested will not add anything substantive to the presentation. Make sure that your argument demonstrates understanding of the subject area, and you provide evidence to justify this point of view. Here are a couple of examples:  C2.1: The drawback is that the illustration concerns very simple system with a low number of state and the extension of the calculation to the case of a 3D humanoid robot is not trivial. For example the calculation of the basin of attraction for a large system is not obvious. It is often difficult to explain clearly new idea in the case of complex system because the complexity can hid the interest of the strategy, thus this present form is convenient but I would appreciate more detailed calculation in the appendix. We agree that this is an important topic requiring further investigation. Due to limited space we have not discussed the various possible approaches by which this method could be scaled up to more complex systems. In the work of Ames, Gregg and Spong 2007, the complexity is resolved by dividing the system into many components which are coupled together through mechanical coupling. Another approach is to treat additional degrees of freedom as noise and discard them. We do not yet know the best approach, but it will certainly be the subject of future work. C3.3: My main complaint is the vague reference to the actual deformable model used in the paper, and the constant confusion between Laplacian editing, LSmeshes and other (nonlinear) surface representations. This opens up an interesting academic discussion, even if it is about semantics. We have defined Least squares meshes in the introduction as anchor points and a matrix encoding the differential and topological properties of the surface (slightly changed from original version), which extends the LS mesh definition of Sorkine and Cohen-Or [5], which is a planar graph with arbitrary connectivity and a sparse set of control points with geometry. Effectively, we consider a LS mesh as a form which can be deformed (the representation) using Laplacian mesh editing (the action). Using this definition enables us to use alternative differential operators with our reconstruction. Section 4 has been added to explain what operators are used when and why. Disagree proactively: This is the situation when the reviewer misunderstands some aspect of your work and do not feel that this needs to be clarified further in your text. You need to phrase your response in a manner that is authoritative but not critical and also must remain respectful of their expertise.  C3.5: Last paragraph of section A. You wrote that passive dynamic models use significantly lower energy compared with ZMP-based walkers like ASIMO. Lower passive dynamic models have less joints than the ASIMO I believe. The differences in the published numbers of the cost of transport of ZMP robots vs passive dynamic models are quite stark, even taking the number of joints into account. BigDog, for example, uses between 15 and 30 times the amount of energy to the Cornell Ranger, which is still not as efficient as a real human. The lower energy consumption relates to the highly efficient design of the passive walker rather than the number of joints. The passive biped of Collins and Ruina [12] has both knees and arms, and consumes no energy when walking downhill. A dismissive response: While this should rarely be used, you do occasionally have to respond to a reviewer that may not have read your article, knows nothing about the area, or is being unprofessional in their assessment. I believe that a polite beat-down is required in these cases, establishing the ignorance of the reviewer but also your authority in the irrelevance of their comment. This could be controversial, but I think it may assist the editor in identifying reviewer comments which are simply not worth the time of day. Here are some of my favourites:  C4.1: There are many claims which are not justified or are incorrect. It is not clear which claims the reviewer is referring to, so we are unable to provide rebuttal or corrections relating to this point. C4.2: There is no demonstration of the system and results are largely anecdotal and unconvincing. We demonstrated our method with 3 different dynamic models. These are not anecdotal, but are genuine dynamic models used to drive and balance existing robots. In addition, we have justified our methodology by evaluating the energy efficiency and the stability of the dynamic system, and by comparison with the motion adaptation strategy of a real human walker. C4.3: The methodology is unclear with no mention of the actual structure of the animated models used and how they are driven. In the Applications section we clearly define the dynamic model for the bouncing ball. The walking model is a modified version of the model of Chen [9], as explained in the paper. ...and on it goes. Sources of Evidence At the review stage, only the following forms of evidence would really be admissible, in decreasing order of authority:\n An existing publication: A reputable publication is probably your best form of evidence when refuting a reviewer argument. This might not necessary be from a directly related subject area, any may be outside of the reviewers discipline area. Here\u0026rsquo;s a really good example:  C3.3: Under Neural Computation, you wrote \"This makes it impossible for the neural system to carry out the complex computation necessary for real-time optimisation\". Really? This is an agreed hypothesis amongst the biological motor control community. Good references for this are the book by Latash and the paper of Nishikawa et al. This is easy to justify: we observe that animals with very simple neural systems are capable of controlling complex motion. Even flagellates (for example, sperm), a class of very simple organisms are capable of a swimming motion by whipping a flagellum (whip-like tail). However, current work in simulating dynamic models which include muscle actuation using optimisation can take many days on large clusters of computers. 2. **A comment from another reviewer**: A good reason to keep positive comments handy is that they can be turned into evidence against another reviewers argument. This is a powerful device for negating a criticism as it plays the reviewers off against each other. Here's a reasonable example: C5.6: Besides, I consider it not necessary to detail so much on the least-squares meshes and Principle Component Analysis in the related work, as they are commonly familiar to the community. These sections were requested by other reviewer(s). They also serve the double purpose of defining terminology used within the paper. 3. **Evidence found within your paper**: Your paper is currently under scrutiny to deduce whether it is sufficient for publication. By citing evidence within the paper you are treading a dangerous path, so ensure that your evidence is strong. In most circumstances, it might be a prudent approach to use the reviewers comments to tighten up your evidence, produce additional results etc in order to prove your case. 4. **Common sense**: I don't think I've ever had to do this. In some guides they recommend that an emotion appeal may be an effective rebuttal strategy, but I would strongly recommend avoiding such an approach for scientific publication. The Cover Letter A cover letter (or email or whatever) is a good opportunity to summarise the general trends of the reviewers, and also to possibly address any of the serious reviewer concerns in a respectful manner. I have also used the cover letter as an opportunity to address any outstanding queries which may arise due to conflict between the reviewers, or space issues. Here is an example:\nWe are thankful to the reviewers comments, which are largely very positive and helpful. Many of their comments are very interesting, questioning fundamental assumptions relating to human and synthetic locomotion. These arise from the multi-disciplinary nature of the work, and we have addressed these by referencing work in complimentary disciplines such as motor control, neurobiology and robotics. The substantive changes we have included are: 1. We have provided a full derivation of the expressions for the local controller parameters (Appendix B), as requested by reviewer 5. In addition, these local controllers are included under the relevant application sections in the main text body. 2. We have included an introduction to Lie group symmetry applied to dynamic systems in Appendix A. 3. We have added a figure demonstrating how the ball would bounce without being coupled to a neural oscillator. It should be noted that amendments to the paper have taken the form of including additional material rather than correcting factual errors. One area of concern is what background needs to be incorporated in a final version to be accessible to the reader. Currently the paper contains an introduction to dynamic systems, which was criticised by Reviewer 1 for being to obvious. Additionally, Reviewer 4 requested a definition of Lie group symmetry, which has necessitated the inclusion of a introduction to the application of Lie group symmetry to differential equations (Appendix A). Depending on space limitations, these sections can be included in the main text, as an appendix, or removed (as adequate references were provided), but they do provide the reader with an accessible and intuitive introduction to these tricky topics, and we hope they can be accommodated in the final text. Conclusion In answer to the question \"Should I Write a Rebuttal\", it is dependent on whether you're able to provide sufficient defence of your article, or are able to revise your article in a manner that meets with the reviewers criteria. I think of writing a paper as trying to plug holes in a dam wall. The first thing an author needs to accept is: there are probably always going to be holes - your results might not contain sufficient examples, your comparisons may not be with the absolutely latest work, your method may not be rigorously proven - although you should obviously do your best to minimise these. The secret of writing the paper is to ensure that there are as few holes as possible, so the reviewers are satisfied that you\u0026rsquo;ve done a good enough job. The purpose of the rebuttal is to plug as many of the holes as possible, from largest to smallest. Even if you choose not to submit a rebuttal (I\u0026rsquo;ve been there) I would strongly recommend going through the exercise above anyway to get a better understanding of how you can improve your research.\nSo in summary:\n Be respectful and authoritative in tone. Read and classify the reviewers comments. Address every negative reviewer comment. Provide evidence in constructing your argument. Provide a broad summary of changes in the cover letter.  ","date":1449446400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1449446400,"objectID":"d9a7c3dc999ff3914fdd88e97b5ce8d9","permalink":"https://rsouthern.github.io/post/rebuttal/","publishdate":"2015-12-07T00:00:00Z","relpermalink":"/post/rebuttal/","section":"post","summary":"I've had some experience in the art of polite persuasion, which I will attempt to share.","tags":["research methods"],"title":"The Rebuttal - Writing a robust defence of a journal submission","type":"post"},{"authors":["min jiang","yahan zhou","rui wang","Richard Southern","jian jun zhang"],"categories":null,"content":"Media    Blue Noise Sampling using an SPH-based Method  from Richard Southern   Source Code   An implementation of multiclass sampling using our SPH sampling method. Implemented in C++ in Visual Studio. Windows binary included. [Download]  An implementation of surface sampling using our SPH sampling method. Implemented in C++ in Visual Studio. Windows binary included. [Download]    ","date":1446940800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1446940800,"objectID":"e9713e24b566b0bc098386a46f428a18","permalink":"https://rsouthern.github.io/publication/sphsampling/","publishdate":"2015-11-08T00:00:00Z","relpermalink":"/publication/sphsampling/","section":"publication","summary":"A novel method for generating blue noise sampling using a fluid simulation framework","tags":["sph","sampling","cuda"],"title":"Blue Noise Sampling using an SPH-based Method","type":"publication"},{"authors":null,"categories":null,"content":"Background Lets say you have a smooth scalar function of any number of variables \\(f:\\mathbb{R}^n\\mapsto\\mathbb{R}^1\\). Let\u0026rsquo;s assume that the function is computationally expensive to evaluate, but you need to do this at run-time, possibly a lot. Let\u0026rsquo;s also assume that you can define the maximum and minimum range of values which you are interested in. Let\u0026rsquo;s also assume that the function is smooth, and you\u0026rsquo;re tolerant of (or even prefer) the results to be smoothed over.\nOne possible solution is to sample values of the function over a regular grid, and perform some sort of interpolation between the values. So we\u0026rsquo;ll define a different function \\(g:\\mathbb{R}^n\\mapsto\\mathbb{R}^1,,g\\approx f\\). Depending on the desired smoothness of the approximation different interpolation functions are feasible, but not all of these generalise to any dimension. One method which does, however, is the Catmull-Rom Spline (co-invented by the CG superhero Ed Catmull) which can be evaluated recursively on grid edges and exhibits \\(C^2\\) continuity.\nI\u0026rsquo;ll explain how this works with the help of this figure, representing a 2D scalar field:\n In this case, we're looking at a function $f:\\mathbb{R}^2\\mapsto\\mathbb{R}^1$ with the value returned by this function visualised as a height above the $(x,y)$ grid in the above image. We assume that the red points are the pre-computed values for $f$ at the corners of the grid cells. Once the cell in which the value to be calculated is identified, 4 Catmull-Rom interpolations are performed in the $y$ direction, to find the green points. One further Catmull-Rom interpolation is needed between these points to compute the final interpolated value (in blue). Note that the number of curves that need to interpolated in each dimension is multiplied by 4 in each step (so on a 3D grid, the first step requires 16 interpolations in the $z$, then 4 in the $y$ and 1 in the $x$). You can see that this process is ripe for recursion. It is also ripe for parallel computation, but that is another story... Applications You might be wondering when you might you want to use an application like this? I developed it in the context of a sparse, high dimensional Moving Least Squares projection: if you are projecting onto a point cloud that is irregular, you need an adaptive, variable radius function in order to ensure that you have sufficient local points on which to project. As it\u0026rsquo;s expensive to compute local neighbours at run time (especially in, say, 66 dimensions) it\u0026rsquo;s probably a good idea to define an approximating smooth function which, given any point in space, returns a radius that will probably contain a desired number of point samples.\nImplementation There are a number of features used in this implementation, which I\u0026rsquo;m going to unpick from the implementation and discuss separately.\nPassing around functions Fundamental to this process is the ability to evaluate a generic function at a point. It would be very convenient for us to be able to pass said function around as a generic function pointer. For a simple generic function signature, I\u0026rsquo;ll use a function that accepts an array of doubles, and returns a double. This can be stated using the std::function method:\nstd::function\u0026lt;double (double*)\u0026gt; _func;  Note that the type notation is C notation for a function that accepts a pointer to a double, and returns a double. This makes _func a generic function that can be passed as a parameter - particularly convenient if we\u0026rsquo;re allowing the use of arbitrary input functions. Of course, the return type and array size could be specified as template parameters, but for interaction with existing functions (e.g. libnoise) I\u0026rsquo;ve kept it simple.\nTemplates and C++11 I do, however, use templates for the actual ScalarField class. In an early implementation of this structure I just used dynamic arrays of doubles to store the data - but this gives us the overhead of dynamic allocation and deallocation, along with the risks associated with non-contiguous memory. C++11 introduces the std::array, which is essentially an old skool C static array, but with all the handy goodness of the STL.\nThis is from the top of the ScalarField class:\ntemplate \u0026lt;uint DIM\u0026gt; class ScalarField { public: /// Unsigned int array typedef std::array\u0026lt;uint, DIM\u0026gt; uintArray; /// Double array typedef std::array\u0026lt;double, DIM\u0026gt; doubleArray;  The template parameter is evaluated at compile time, generating a template class for the desired dimension. Whenever I create an object of type doubleArray in the code, this is done statically.\nNote that I could have given a template parameter for a real type and an integer type, but that will add some unnecessary complexity for now.\nRecursive Algorithm The core of the algorithm is the construction of the grid and the recursive evaluation of the query point. Fortunately these are reasonably similar. Lets have a look at the init routine:\n/** * @brief ScalarField::initialise Inits the scalar field * @param _func A boost-ified function declaration which returns the scalar value * @param _minValue The minimum values of our given field. * @param _maxValue The maximum values of our given field. * @param _resolution The resolution of the field in each dimension. */ template\u0026lt;uint DIM\u0026gt; void ScalarField\u0026lt;DIM\u0026gt;::init(std::function\u0026lt;double (double*)\u0026gt; _func, const doubleArray \u0026amp;_minValue, const doubleArray \u0026amp;_maxValue, sconst uintArray\u0026amp; _resolution) { // Check we've not been here before already if (m_isInit) cleanup(); // Precompute the partition size in each case // Now we can allocate some memory for our data // Now for each value in the field we can use the provided function to derive the scalar value // this is done recursively (could easily be done in parallel doubleArray pos; uintArray idx; initRecurse(_func, 0u, idx, pos); // Set this to initialised m_isInit = true; }  Not very much excitement there: we set up the memory and ready ourselves for running the recursive function. The initRecurse function is obviously where the action is - so I\u0026rsquo;ll break it down into stages:\n/** * @brief ScalarField::initRecurse Recursively initialise the scalar field along each dimension. * @param _func The boostified function to execute to determine the scalar value * @param dim The current dimension (start this at 0) * @param idx The current index to evaluate (empty when you start) * @param pos The position to evaluate (empty when you start) */ template\u0026lt;uint DIM\u0026gt; void ScalarField\u0026lt;DIM\u0026gt;::initRecurse(std::function\u0026lt;double (double*)\u0026gt; _func, const uint \u0026amp;dim, uintArray \u0026amp;idx, doubleArray \u0026amp;pos) {  The prototype reveals a lot about the functionality of the function. The first parameter is the function that we want to call at the grid points. The second parameter is the current dimension. You\u0026rsquo;ll notice from the code above that this starts at 0 - each time the function is called, the dimension number will tick up until it gets to the actual dimension, which is our recursion termination criteria. Index and the position values fill up after the recursion unrolls.\n// At the end of the recursion chain we can calculate the value using the function if (dim \u0026gt;= DIM) { // First determine the index in the data ... // Now set the value of the data in the right place to the result of the function m_data[dataIdx] = _func(\u0026amp;pos[0]); return; }  At the end of the recursion chain, we evaluate the given function at the position. This isn\u0026rsquo;t straightforward, as I\u0026rsquo;ve stored all the data in a big flat 1D array. However I\u0026rsquo;ve removed the details for simplicity.\nIf the function didn\u0026rsquo;t return, we know that we\u0026rsquo;re not at the end of the chain so we recursively evaluate across each grid dimension:\n// If we didn't get to the end of the chain, we run another iteration for (i = 0 ; i \u0026lt; m_resolution[dim]; ++i) { idx[dim] = i; pos[dim] = m_minValue[dim] + i * m_partSize[dim]; initRecurse(_func, dim+1, idx, pos); }  In this way, each node in each dimension is initialised with our input function.\nThe evaluation function is a bit more tricky:\n/** * @brief ScalarField::eval Evaluate the scalar value at the specified point. * @param pt The position at which to evaluate the scalar field * @return */ template\u0026lt;uint DIM\u0026gt; double ScalarField\u0026lt;DIM\u0026gt;::eval(const doubleArray\u0026amp; pt) const { if (!m_isInit) return 0.0; // Determine the index of the bottom corner of the evaluation grid (if outside, extrapolation // may be used, so it's capped at the bounds of the grid) ... // Deduce the index of the points from the min, max grid values and total resolution ... // Evaluate the position recursively over all dimensions double ret_val = evalRecurse(0, idx, currentIdx, x); // Clear memory and return return ret_val; }  This is largely the same as the init function above, except that you will need to calculate the bottom corner index of the grid cell in which the query point lives (which is messy, and hidden away).\nThe header for evalRecurse is more or less the same as initRecurse so I won\u0026rsquo;t repeat myself. The big difference, of course, is that this function actually returns a value. Here\u0026rsquo;s the termination condition:\n// The termination condition - just return the scalar value at the specified position if (dim \u0026gt;= DIM) { // First determine the index in the data ... // Just return the data value at this position! return m_data[dataIdx]; }  This just returns the value at the grid point - this makes more sense in the context of the general condition below:\nelse { // Evaluate the catmull rom spline recursively by combining the current indices // along all dimensions double v0, v1, v2, v3; currentIdx[dim] = idx[dim] + 0; v0 = evalRecurse(dim+1, idx, currentIdx, x); currentIdx[dim] = idx[dim] + 1; v1 = evalRecurse(dim+1, idx, currentIdx, x); currentIdx[dim] = idx[dim] + 2; v2 = evalRecurse(dim+1, idx, currentIdx, x); currentIdx[dim] = idx[dim] + 3; v3 = evalRecurse(dim+1, idx, currentIdx, x); return catmullRomSpline(x[dim], v0, v1, v2, v3); }  So this function spawns 4 different evalRecurses for this thread, each one potentially spawning another 4 ad nausium. The results of each thread re interpolated using our magical catmullRomSpline function (which is really easy - check the code).\nConclusions This is a nice example of an bit of code that is useful for teaching and research: there are a couple of nice C++11, C++ and C principles at play which greatly simplify the implementation and improve performance. The research behind it pretty nifty and broadly applicable (I would think).\nYou may have observed that you could use this approach for general dimensional mapping, i.e. $f:\\mathbb{R}^n\\mapsto\\mathbb{R}^m$, assuming that you create m different ScalarFields. Vector Field intepolation might be more difficult as they will require some recombination function (e.g. normalisation), but not impossible.\nParallelism is an obviously missing thing here: in very high dimensional data sets this could prove to be particularly useful - it is essentially a scan-reduce function. Send me an email if you want this and I can check this out.\nDownloads The source code is hosted on GitHub. Clone it from the repository using the following command from your console:\ngit clone https://github.com/rsouthern/Examples  ","date":1442620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442620800,"objectID":"71e2f73622c65f4f7d03083d5febd967","permalink":"https://rsouthern.github.io/post/scalarfield/","publishdate":"2015-09-19T00:00:00Z","relpermalink":"/post/scalarfield/","section":"post","summary":"A recursive implementation of N-Dimensional Catmull Rom spline interpolation for general scalar fields in C++.","tags":["geometry processing","parameterization","c++"],"title":"Recursive n-dimensional scalar field interpolation","type":"post"},{"authors":["min jiang","Richard Southern","jian j zhang"],"categories":null,"content":"Winner of NVIDIA Best Paper and Best Student Paper awards!\nPaper A preprint of the paper is available [here].\nMedia    ","date":1426032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426032000,"objectID":"2533741aa9ccfb02cf5753f628ee1133","permalink":"https://rsouthern.github.io/publication/dissolution/","publishdate":"2015-03-11T00:00:00Z","relpermalink":"/publication/dissolution/","section":"publication","summary":"A method for simulating dissolution and erosion","tags":["sph","fluid","simulation"],"title":"A Particle-based Dissolution Model using Chemical Collision Energy","type":"publication"},{"authors":["shihui guo","Richard Southern","jian chang","david greer","jian j zhang"],"categories":null,"content":"","date":1393632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393632000,"objectID":"0a0554365cb55424e0bb71d43b155c0e","permalink":"https://rsouthern.github.io/publication/motionsurvey/","publishdate":"2014-03-01T00:00:00Z","relpermalink":"/publication/motionsurvey/","section":"publication","summary":"A survey of adaptive motion synthesis techniques","tags":["survey","simulation","dynamic systems"],"title":"Adaptive Motion Synthesis for Virtual Characters - A Survey","type":"publication"},{"authors":null,"categories":null,"content":"Note - while this talk was tailored for Engineering Doctoral students, it is equally relevant to PhD study. I woul recommend familiarising yourself with this material well in advance of your transfer exam, ideally shortly after enrolment.\n  A Survival Guide to the Transfer Viva  from Richard Southern  ","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"a74c8bd425eea76e376bb9c8abee1947","permalink":"https://rsouthern.github.io/post/hypothesis/","publishdate":"2014-01-01T00:00:00Z","relpermalink":"/post/hypothesis/","section":"post","summary":"A presentation I gave on how to better understand this crucial milestone in doctoral study.","tags":["research methods"],"title":"A Survival Guide to the Transfer Viva","type":"post"},{"authors":null,"categories":null,"content":"This is a talk I\u0026rsquo;ve given a few times on the uses of Virtual Reality in Behavioural Psychology research. It documents our work on the Bystander Effect, published at dx.doi.org/10.1371/journal.pone.0052766 . The argument I put forward is that Virtual Reality is useful for experiments where reality presents ethical or logistical challenges. Also I\u0026rsquo;ll talk about some future directions if you can hold out until the end.\n Virtual Reality in Behavioural Psychology: Talk from Richard Southern on Vimeo.\n","date":1374796800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374796800,"objectID":"6d3552a4b04c915dbfd580f826d67856","permalink":"https://rsouthern.github.io/talk/bystander/","publishdate":"2013-07-26T00:00:00Z","relpermalink":"/talk/bystander/","section":"talk","summary":"This is a talk I've given a few times on the uses of Virtual Reality in Behavioural Psychology research.","tags":["presentation","dynamical systems","robotics"],"title":"Virtual Reality in Behavioural Psychology","type":"talk"},{"authors":["fangde liu","Richard Southern","shihui guo","xiaosong yang","jian j zhang"],"categories":null,"content":"Media       A Theory for Motion Primitive Adaptation  from Richard Southern   ","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"b5a9d15f67a6fa415dac8cec96ac1f84","permalink":"https://rsouthern.github.io/publication/motorinvariant/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/motorinvariant/","section":"publication","summary":"We present a theory for motion primitive adaptation derived from robotics","tags":["simulation","motion","dynamic systems"],"title":"Motion Adaptation With Motor Invariant Theory","type":"publication"},{"authors":null,"categories":null,"content":"In the real world you often need to solve really big, spatial boundary value problems. There are a couple of examples of this:\n Solutions to general Partial Differential Equations Finite Element Methods (solving heat equations, antennae simulations, deformation) Fluid Simulations (Eulerian and Legrangian methods) Radial Basis Functions (smooth data interpolation)  The basic form for these systems is often the same. $f(x) = \\sum_i q_i \\Phi(| x - x_i |)$ where $\\Phi(r)$ is some smooth kernel function and $x_i$ is an interpolation center / observation site.\nThe slides for the associated talk are here:\n  GPU Accelerated Domain Decomposition  from Richard Southern  A rather out of date implementation is available on Sourceforge.\n","date":1364860800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1364860800,"objectID":"ebf003a3cc682ba2dd62141412d3aea7","permalink":"https://rsouthern.github.io/post/domain/","publishdate":"2013-04-02T00:00:00Z","relpermalink":"/post/domain/","section":"post","summary":"A simple CUDA accelerated solver for general dimensional n-body problems. Currently only the conjugate gradient method has been implemented, but more will soon follow.","tags":["cuda","solver"],"title":"GPU Accelerated Domain Decomposition","type":"post"},{"authors":["aitor rovira","david swapp","Richard Southern","jian j zhang","mel slater"],"categories":null,"content":"Media    ","date":1363564800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1363564800,"objectID":"6ba82572be37c681d66315ca139f3f56","permalink":"https://rsouthern.github.io/publication/projectors/","publishdate":"2013-03-18T00:00:00Z","relpermalink":"/publication/projectors/","section":"publication","summary":"A study of the effect of projector displays on the bystander experiment","tags":["psychology","vr"],"title":"The impact of enhanced projector display on the responses of people to a violent scenario in immersive virtual reality","type":"publication"},{"authors":["xiaosong yang","jian chang","Richard Southern","jian j. zhang"],"categories":null,"content":"Media    @article{ year={2013}, issn={0178-2789}, journal={The Visual Computer}, volume={29}, number={5}, doi={10.1007/s00371-012-0739-3}, title={Automatic cage construction for retargeted muscle fitting}, url={http://dx.doi.org/10.1007/s00371-012-0739-3}, publisher={Springer-Verlag}, keywords={Muscle modelling; Character animation}, author={Yang, Xiaosong and Chang, Jian and Southern, Richard and Zhang, JianJ.}, pages={369-380}, }  ","date":1362096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362096000,"objectID":"bf504e1f1a572659fcdce0bfe66bd68f","permalink":"https://rsouthern.github.io/publication/cagebased/","publishdate":"2013-03-01T00:00:00Z","relpermalink":"/publication/cagebased/","section":"publication","summary":"A cage-based method for transfering muscles between characters.","tags":["geometry processing","graphics"],"title":"Automatic cage construction for retargeted muscle fitting","type":"publication"},{"authors":["mel slater","aitor rovira","Richard Southern","david swapp","jian j. zhang","claire campbell","mark levine"],"categories":null,"content":"Media        Virtual Reality in Experimental Psychology  from Richard Southern   Press  BBC London 2013 Reuters 2013 BBC News 2010  ","date":1357084800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1357084800,"objectID":"8f6adbed7893b5783b48ad570f7aae94","permalink":"https://rsouthern.github.io/publication/bystander/","publishdate":"2013-01-02T00:00:00Z","relpermalink":"/publication/bystander/","section":"publication","summary":"An experiment in virtual reality to identify the role of group affiliation in the bystander effect","tags":["vr","psychology"],"title":"Bystander Responses to a Violent Incident in an Immersive Virtual Environment","type":"publication"},{"authors":["Richard Southern","jian j. zhang"],"categories":null,"content":"Media    ","date":1306886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1306886400,"objectID":"2fd79c2e35ec44150c1c3dc2de3e66ff","permalink":"https://rsouthern.github.io/publication/keypoints/","publishdate":"2011-06-01T00:00:00Z","relpermalink":"/publication/keypoints/","section":"publication","summary":"A statistical method to identify key deformation anchors","tags":["dimensional reduction","geometry processing"],"title":"Motion-Sensitive Anchor Identification of Least-Squares Meshes from Examples","type":"publication"},{"authors":null,"categories":null,"content":"The method of force density was developed in response to the need for computational modelling of structures for the Munich Olympic complex [Lewis 2013] . The method relies on the assumption that the ratio of tension force to length of each cable can be constant, transforming a system of non-linear equations to a set of linear equations which can be solved directly.\nThe Force Density Method (FDM), first introduced by Scheck [1974], is commonly used in engineering to find the equilibrium shape of a structure consisting of a network of cables with different elasticity properties when stress is applied. While shape analysis of tensile structures is a geometrically non-linear problem, the FDM linearises the form-fitting equations analytically by using the force density ratio for each cable element, \\(q = F/L\\), where \\(F\\) and \\(L\\) are the force and length of a cable element respectively.\nThe Force Density Method Given the positions of nodes (vertices) which connect the cables (edges) of our network \\(V\\), the topology of this network is encoded in the branch-node matrix \\(C\\). The branch-node matrix of the network above is given by: Given a load vector \\(R\\) and the diagonal matrix of force densities \\(Q\\), the equilibrium location can be deduced by solving for \\(X\\) in\n$$(C^T Q C) X = R.$$\nEmbedding In order to use the FDM for 2D embedding, we must be able to constrain nodes on the boundary. For this, the matrix \\(C\\) is separated into two sub\u0026ndash;matrices: \\(C_f\\) contains constrained nodes with corresponding position \\(X_f\\), while \\(C\\) contains those that are free to move. The problem is reformulated as $$(C^T Q C) X = R - (C^T Q C_f)X_f.$$\nNote that for the purposes of embedding, \\(R\\) is typically set to zero.\nThe FDM is fold-over free. This is explained with reference to the following Figure:\n  As the natural rest internal force load of each node is exactly 0, any foldover will result in external forces applied to the nodes which have folded over. As a result it will not be in a state of equilibrium, and this configuration cannot occur. Stability under motion The FDM cannot guarantee any of the commonly advocated properties of embeddings in computer graphics, such as isometry (length preserving) or conformality (angle preserving). In our application, it is desirable that the path of a vertex or a group of vertices moving across the surface of the shape is mirrored in the embedding. We evaluate this phenomenon by measuring the distortion of these displacement vectors in the embedded space. We call this property stability under motion.\n       In the above Figure we compare two popular fixed boundary conformal techniques, Harmonic mappings [Eck at al. 1995] and Mean Value Coordinates [Floater 2003] with an FDM network constructed with unit edge forces. The stability under motion of these embedding methods is highly non-linear, and so we evaluate each embedding technique experimentally as follows:\n Compute the embedding \\(\\mathcal{U}_0 = \\mathsf{embed}\\left(\\mathcal{M}_0\\right)\\). Rotate a set of points on the sphere (in this case, one triangle) in a straight path around the surface of the sphere. For each state of the rotation \\(\\mathcal{M}_i\\) compute \\(\\mathcal{U}_i=\\mathsf{embed}\\left(\\mathcal{M}_i\\right)\\). Compute the \\(n\\) displacement vectors in the embedded space. For each \\(u_{0,i} \\in \\mathcal{U}_O\\) and \\(u_{d,i} \\in \\mathcal{U}_d\\) define \\(\\mathbf{d}_i\\) as the vector between \\(u_{0,i}\\) and \\(u_{d,i}\\). Let \\( D = \\left[\\mathbf{d}_1, \\ldots , \\mathbf{d}_n\\right]^T \\). We use the angle of the normal cone of these displacement vectors $$\\alpha = \\max_{i,j} \\left(\\mathrm{acos}(\\mathbf{d}_i \\cdot \\mathbf{d}_j)\\right)$$ as the distortion error. For this experiment, we evaluate the distortion of only the points moving on the surface.  We demonstrated that under all rotations the FDM embedding is stable, even when faces of \\(\\mathcal{M}_d\\) overlap. In addition, there is considerably less displacement of surrounding nodes after rotations.\nConclusion I have presented the force density method as a technique to perform mesh embedding using this technique. It is computationally efficient, as it only involves the solution to a sparse linear system, easily solved with the Conjugate Gradient Method or using Cholesky Factorization. I demonstrated that embedding with FDM is very stable, preventing foldover and discontinuities when parameterizing an unstable triangulation. This is particularly useful when, for example, you need to flatten geometry in a stable fashion (for example, see Zhang et al. [2007] and our paper on skin sliding).\nBibliography Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. ACM Transactions on Graphics (Proceedings of SIGGRAPH \u0026lsquo;95). 173-182 doi: 10.1145/218380.218440.\nMichael S. Floater, Mean value coordinates, Computer Aided Geometric Design, 20(1):19-27, 2003, ISSN 0167-8396, 10.1016/S0167-8396(03)00002-5.\nWanda J. Lewis. Tension Structures: Form and Behaviour. Thomas Telford, illustrated edition, 2003. ISBN 0727732366, 9780727732361.\nJ. H. Schek. The force density method for form finding and computation of general networks. Computer Methods in Applied Mechanics and Engineering, (3):115134, 1974. doi: 10.1016/0045-7825(74)90045-0\nZhang, J. J., Yang, X. and Zhao, Y. (2007), Bar-net driven skinning for character animation. Comp. Anim. Virtual Worlds, 18: 437446. doi: 10.1002/cav.211\n","date":1296518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1296518400,"objectID":"4daa2e0312bb2df863aa3d771a6bd475","permalink":"https://rsouthern.github.io/post/fdm/","publishdate":"2011-02-01T00:00:00Z","relpermalink":"/post/fdm/","section":"post","summary":"FDM is a linear method, commonly used in engineering, which provides stable parameterizations for Computer Graphics applications.","tags":["geometry processing","parameterization"],"title":"The Force Density Method - A Brief Introduction","type":"post"},{"authors":null,"categories":null,"content":"\\( \\newcommand{\\x}{\\mathbf{x}} \\newcommand{\\n}{\\mathbf{n}} \\newcommand{\\q}{\\mathbf{q}} \\newcommand{\\e}{\\mathbf{e}} \\newcommand{\\diag}{\\mathsf{diag}} \\newcommand{\\khachiyan}{\\mathsf{Khachiyan}} \\newcommand{\\flatClust}{\\mathsf{flatClust}} \\newcommand{\\kmeans}{\\mathit{k}\\mathsf{means}} \\newcommand{\\sort}{\\mathsf{sort}} \\newcommand{\\tP}{\\tilde{P}} \\newcommand{\\idx}{\\mathbf{i}} \\newcommand{\\tidx}{\\tilde{\\idx}} \\newcommand{\\List}{\\mathcal{L}} \\newcommand{\\I}{\\mathcal{I}} \\newcommand{\\eig}{\\mathsf{eig}} \\)\nIn this post I present a method to reconstruct a surface representation from a a set of EBF\u0026rsquo;s, and in addition present an efficient top-down method to build an EBF representation from a point cloud representation of a surface. I also discuss the advantages and disadvantages of this approach.\nRadial basis functions (RBFs) are a popular variational representation of volumes and surfaces in computer graphics. In general, an RBF is a real-valued function whose value depends only on the distance from it\u0026rsquo;s center. A special class of these functions have compact support - in this case the function decays smoothly to zero as the radius approaches 1. In this way, only a relatively small number of RBF\u0026rsquo;s influence any particular point in space, which in turn greatly improves computational efficiency.\n      (a) (b)  On certain features (a), basic radial basis functions are inefficient at capturing the surface properties. In comparison (b), very few elliptical functions would be needed to represent this feature.  However, certain features are not best represented by radial basis functions, such as in Figure below. Consider two parallel lines - if they are close enough together, you will need many RBF\u0026rsquo;s in order to ensure that the two features are separated. In comparison, an elliptical shape can better represent this structure (see figure).\nIn this technical report I present a method to reconstruct a surface representation from a a set of EBF\u0026rsquo;s, and in addition present an efficient top\u0026ndash;down method to build an EBF representation from a point cloud representation of a surface. I also discuss the advantages and disadvantages of this approach.\nBackground The reader is probably familiar with the well known general elliptical form \\(x^2 / a^2 + y^2 / b^2 = 1\\). This 2D formulation assumes the ellipse is centered at the origin, \\(a\\) and \\(b\\) are the lengths of the major and minor axes which are aligned with the Cartesian axes. In general we use the quadratic form for an ellipse \\( Ax^2 + By^2 + Cx + Dy + Exy + F = 1 \\). This can be rewritten in quadratic (matrix) form:\n\\begin{equation} f(\\mathbf{x}) = (\\mathbf{x}-\\mathbf{q})^T Q (\\mathbf{x}-\\mathbf{q}) = 1 \\label{eq:quadratic} \\end{equation}\nfor ellipse center \\(\\mathbf{q}\\) and shape matrix \\(Q\\). Note that for real roots, \\(Q\\) must be positive semi-definite, i.e. (\\(Q = Q^T\\) and \\(\\left\\langle x, Qx \\right\\rangle \\geq 0\\) for all \\(x\\in\\mathbb{R}^n\\)). \\(Q\\) can be factorized into \\(Q=M^TM\\).\nAn alternative, more compact formulation is to use homogeneous coordinates \\(\\hat{\\mathbf{x}} = \\left[ \\mathbf{x}, 1\\right]^T\\) and combine \\(\\mathbf{q}\\) and \\(Q\\) into a single matrix with a translational component: \\begin{equation} A = \\left[ \\begin{array}{cc} Q\u0026amp; 0\\\\\n-\\mathbf{q}\u0026amp; 1 \\end{array} \\right] \\end{equation}\nso that quadratic form becomes the homogeneous form for the ellipsoid. \\begin{equation} f(\\hat{\\x}) = \\hat{\\x}^T A \\hat{\\x} = 1 \\label{eq:homogeneous} \\end{equation} In general we refer to the ellipse by the pair \\([\\q, Q]\\). Note that the quadratic and homogeneous forms of the ellipsoid compute the elliptical radius. Note that the volume of an ellipse is given by \\(v = \\sqrt{\\det(Q)}\\).\nRadial Basis Functions Radial Basis Functions (RBF) provide a simple method to construct smooth implicit surfaces from data of arbitrary dimension. Given a matrix of sample points: \\(P = \\left[\\x_1, \\dots,\\x_n \\right]\\) which we assume are generated by sampling on the smooth implicit surface \\( \\hat{f}(\\mathbf{x}) = 0 \\), we estimate this function using the a standard RBF formulation \\begin{equation} f(\\x) = \\sum_{\\q \\in \\mathcal{C}} \\alpha_i \\phi_{\\sigma_i} \\left(\\left\\|\\x - \\q\\right\\|\\right) + \\mathbf{b}^T p(\\x), \\label{eq:rbf} \\end{equation} where \\(\\sigma\\) is the local basis function radius for compactly supported RBFs \\(\\phi_{\\sigma}(r) = \\phi(r/\\sigma)\\), \\(\\phi(r)\\) is a radial basis function, \\(\\mathbf{b}=[\\beta_1,\\ldots,\\beta_{|p(\\x)|}]^T\\), \\(\\alpha_i\\) and \\(\\beta_j\\) are unknown coefficients and \\(p(\\x)\\) is some polynomial in \\(\\x\\) with \\(|p(\\x)|\\) terms (A good choice for \\(p(\\x)\\) is typically \\(\\x + 1\\)). The set \\(\\mathcal{C}=\\left\\{\\q_1,\\ldots,\\q_m\\right\\}\\) contains the chosen (RBF) centers, and for a compact approximation we assume \\(m \\ll n\\).\nThe choice of the basis function \\(\\phi(r)\\) depends on the application - we have used globally supported spline \\(\\phi(r)=r^2 \\log(r)\\), near compactly supported Gaussian \\(\\phi(r)=e^{-r^2}\\) and compactly supported Wendland functions [18] \\begin{equation} \\phi(r)=(1-r)^4_{+}(4r+1) \\end{equation} Note, the \\(f(r)_{+}\\) operator ensures positivity, i.e. if \\(f(r) \\lt 0\\) then \\(f(r)_{+} = 0\\), else \\(f(r)_{+} = f(r)\\).\nDinh and Turk [7] propose the use of the spline formulation of Chen and Suter [6] due to the ability to locally control the smoothness of the resulting surface. This formulation requires two additional smoothness parameters which must currently be chosen in an ad-hoc fashion. As we will define locally anisotropic basis functions, the derivation of locally adaptive variants of \\(\\phi\\) adds an unnecessary layer of complexity that is best avoided.\nVariational Implicit Surface Approximation RBF\u0026rsquo;s have been used extensively for the interpolation of volumetric data, neural networks and smooth surface approximations [17,3]. For surface approximation, a subset of \\(k\\) input points are chosen as RBF centers are chosen from the input data \\(\\mathcal{C}_s\\), and \\(l\\) additional centers are added which are known to be on the exterior of the object \\(\\mathcal{C}_e\\), \\(\\mathcal{C}=\\mathcal{C}_s \\cup \\mathcal{C}_e\\).\nWe use the fact that\n\\begin{equation} f(\\q) = \\left\\{ \\begin{array}{rl} 0,\u0026 \\q \\in \\mathcal{C}_s\\\\\\\\ -1,\u0026 \\q \\in \\mathcal{C}_e \\end{array} \\right. \\end{equation}\nin order to evaluate the coefficients \\(\\alpha_i\\) using linear regression. The problem can be stated in matrix form\n\\begin{equation} \\left[ \\begin{array}{cccc} \\phi_{1,1}\u0026\\cdots\u0026\\phi_{1,m}\u0026p(\\q_1)\\\\\\\\ \\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots\\\\\\\\ \\phi_{m,1}\u0026\\cdots\u0026\\phi_{m,m}\u0026p(\\q_m)\\\\\\\\ p(\\q_1)^T\u0026\\cdots\u0026p(\\q_m)^T\u0026\\mathbf{0} \\end{array} \\right] \\left[ \\begin{array}{c} \\alpha_1\\\\\\\\ \\vdots\\\\\\\\ \\alpha_{k+l}\\\\\\\\ \\mathbf{b} \\end{array} \\right] = \\left[ \\begin{array}{c} f(\\q_1)\\\\\\\\ \\vdots\\\\\\\\ f(\\q_{k+l})\\\\\\\\ \\mathbf{0} \\end{array} \\right], \\end{equation}\nwhere \\(\\phi_{i,j} = \\phi_{\\sigma_i}(\\left|\\q_i - \\q_j\\right|)\\). Using the linear system above we can solve for the coefficients \\(\\alpha_i\\) and \\(\\mathbf{b}\\), and using these the implicit surface can be evaluated at any point using the RBF formulation.\nDefining the locations of external centers \\(\\mathcal{C}_e\\) requires some concept of the orientation of the surface. Often [14,15,17] an associated normal field is assumed. In these cases, an external center \\(q_e\\) is simply defined in terms of the center on the surface \\(\\q_s\\), \\(\\q_e\\) = \\(\\q_s\\) + \\(\\psi \\n_s\\), for some \\(\\psi \u0026gt; 0\\).\nElliptical basis functions      Anisotropic radial basis functions compute distances in the warped space, computed by applying the transformation matrix M.    The isotropic behavior of RBF interpolation and resulting smoothness is often not a desirable property. Consider the bunny\u0026rsquo;s ear in the figure. Because a single RBF center with a large \\(\\sigma_i\\) is used to represent the flat part of the ear, the reconstruction does not reproduce this flat region. This problem could be solved by using many smaller centers to encode the flat region, but this can dramatically increase the size of the above linear system, making the problem expensive to solve.\nFor flat oriented regions an ellipse better approximates shape. Recall that the shape matrix ((Q\\) describes the shape of the ellipse. In particular, because \\(Q\\) is positive semi-definite and symmetric, we can factorize it using singular value decomposition (in MATLAB, \\([V, \\lambda] = \\eig(Q)\\), \\(M = V \\diag(\\sqrt{\\lambda})V^T\\)) to solve it \\(Q=M^T M\\). In this figure we warp the input space by transforming the input points using the ellipse shape matrix, i.e. \\(\\x\u0026rsquo; = M(\\x-\\q)+\\q\\). This space warping procedure is a method for local anisotropic interpolation.\nFormulation For an elliptical basis function formulation we define our set of centers as\n\\begin{equation} \\mathcal{C}=\\left\\{[\\q_1,Q_1,\\sigma_1], \\ldots, [\\q_m,Q_m,\\sigma_m]\\right\\}, \\end{equation}\nconsisting of tuples containing the elliptical information. We can incorporate this local space warping matrix \\(M\\) in the RBF definition: \\begin{equation} f_k(\\x) = \\sum_{\\q \\in \\mathcal{C}} \\alpha_{i,k} \\phi_{\\sigma_i} \\left(\\left|M_k (\\x - \\q)\\right|\\right) + \\mathbf{b}_k^T p(\\x). \\end{equation} Note that we use a subscript \\(k\\) to denote which transformation function is used. The coefficients must now be computed for each EBF center (and hence each \\(M_k\\) using the linear system.\nThe problem of locally anisotropic RBF\u0026rsquo;s is resolved using a partition of unity approach. Loosely speaking, the coefficients \\(\\alpha_i\\) and \\(\\beta_j\\) are deduced for each of the elliptical centers \\([\\q,Q,\\sigma]\\in \\mathcal{C}\\). In order to evaluate an isovalue at some \\(\\x\\), we compute a weight based on the proximity of \\(\\x\\) from each center in the locally warped space. Then, the final isovalue is computed by computing the sum of these locally computed weighted functions.\nMore formally, we compute the isovalue by defining a new combined isosurface function \\begin{equation} g(\\x) = \\frac{\\sum_{k=1}^{m} w_k(\\x) f_k(\\x)}{\\sum_{k=1}^{m} w_k(\\x)} \\label{eq:combined} \\end{equation} with the isosurface at \\(g(\\x)=0\\). By choosing a smooth weight function \\(w_k(\\x)\\) we ensure that the reconstruction results of the combined isosurface function is also smooth. Casciola et al. [4] use the local weight function \\begin{equation} w_k(\\x) = \\left(\\frac{\\left(\\sigma_k - |M_k(\\x - \\q_k)|\\right)_{+}}{\\sigma_k |M_k(\\x - \\q_k)|}\\right)^{\\gamma_k}, \\label{eq:weights} \\end{equation} where \\(\\sigma_k\\) is, the region of influence of each local anisotropic center and \\(\\gamma_k\\) is a local regularization exponent. We have used \\(\\gamma_k=1\\) for all our results. Note that \\(\\sigma_k\\) here is used to both scale the radius in the EBF equation and to determine the weights, and is a measurement of the region of influence of a compact elliptical basis function.\nSo in summary, given a set of elliptical centers \\(\\mathcal{C}\\) consisting of the position \\(\\q\\), shape matrix \\(Q\\) and radius of influence \\(\\sigma\\) of each center, we construct a variational implicit surface using elliptical basis functions as follows:\n For each center \\([\\q_k,Q_k,\\sigma_k]\\in\\mathcal{C}\\), compute the coefficients \\(\\alpha_{i,k}\\), \\(\\mathbf{b}_k\\) using the linear system as a preprocess. For an input point \\(\\x\\), compute each of the weights \\(w_k\\). Compute \\(g(\\x)\\) using these weights in using the EBF formulation and the combined isosurface function.  Building an EBF surface from point data In this section we focus on the construction of EBF surfaces from point cloud data in any dimension without any shape information, such as surface normals. In order to construct an EBF surface we need a number of components:\n The elliptical shape properties of each center \\(\\q_k\\) and \\(Q_k\\), The radius of influence of each center \\(\\sigma_k\\), A normal field for the determination of external centers \\(\\mathcal{C}_e\\), and Some radial basis function \\(\\phi(r)\\).  For our application, we choose the radius of influence arbitrarily as the minimum radius needed to enclose a user specified number of neighboring centers in the warped elliptical space. For the radial basis function \\(\\phi(r)\\) we make use of one of the standard RBF functions, depending on the application. In the following sections we will present a method for geometrically identifying the EBF centers and the the local region of influence for each center, as well as discuss our method to deduce the location and orientation of the elliptical centers.\nFlatness clustering Other authors have made use of either randomized [7] or bottom-up [4] approaches to selecting surface centers. Unfortunately these either yield unpredictable results, or are expensive because of the need to compute local curvature information at every input point.\n     Constructing EBF's over a 3D point cloud.    We define a recursive top-down algorithm for partitioning an input set of points \\(P\\) into flat regions. Loosely speaking, we compute a minimum volume ellipse from the current list of points and measure the flatness. We measure the \u0026ldquo;flatness\u0026rdquo; by using the ratio of the minimum ellipse axis length over the sum of all elliptical axis lengths, similar to the the method of Luiz et al. [13]. If the surface is not sufficiently flat we subdivide the list of points by using a standard clustering algorithm, and append the results of recursive calls to the same function on each cluster.\nThe \\(\\flatClust\\) algorithm makes use of the Khachiyan method for finding the minimum volume ellipse \\(\\khachiyan(P, \\varepsilon)\\), further discussed in the appendix. The eigenanalysis function \\(\\eig\\) returns both the eigenvectors \\(V\\) and eigenvalues \\(\\lambda\\). \\(\\kmeans(P,\\idx)\\) uses the method of Lloyd [12] to cluster only the points in \\(P\\) with the indices \\(\\idx\\), and returns the set \\(\\I = \\left\\{ \\tidx_1, \\ldots, \\tidx_n \\right\\}\\) with each \\(\\tidx_j\\) containing the indices of \\(P\\) belonging to each of the \\(n\\) clusters. We have used \\(n=2\\) for best results, although convergence is often faster when using a larger number of clusters. This approach can easily be applied to 3D data, as in the figure above.\nConsistent orientation In order to determine the external elliptical centers \\(\\mathcal{C}_e\\) we require a local surface normal \\(\\n_s\\). We can easily deduce an unoriented normal from the eigenvector of \\(Q\\) associated with it\u0026rsquo;s smallest eigenvalue.\nA popular method for orienting these normals is by using the propagation method of Hoppe et al. [9]. In brief, this method constructs a Riemannian graph by defining each normal (tangent plane) as the nodes and edges connecting them are deduced using some proximity metric (in [9] this is the distance between the centers). A cost associated with an edge connecting node \\(N_i\\) to \\(N_j\\) is defined as \\(1-|\\n_i\\cdot \\n_j|\\). The tree is traversed with a minimal spanning tree [16]. Whenever an edge \\((i,j)\\) is traversed, the orientation of \\(\\n_i\\) is corrected if \\(\\n_i \\cdot \\hat{\\n}_j \\lt 0\\), where \\(\\hat{\\n}\\) has already been corrected.\nIn order to approximate the Riemannian graph, and thereby reduce the computation time and errors arising from using a minimal spanning tree, we instead determine neighboring centers by using ellipse intersection. Traditional ellipse intersection techniques require computing the roots of a quadratic polynomial, which can be time-consuming to compute numerically.\nAlfano and Greer [1] present a method to test for the intersection of two ellipses \\(A\\) and \\(B\\) (in the homogeneous form). The roots of the intersection can be found by determining \\([V,\\lambda] = \\eig(A^{-1} B)\\) and testing eigenvectors associated with non-real or repeated eigenvalues. This approach is easy to implement and very efficient as \\(A^{-1}\\) can be precomputed for all ellipses.\nConsolidation Because \\(\\kmeans\\) clustering is not flatness sensitive, flat regions may become fragmented due to this procedure. An additional consolidation step is required to merge neighboring elliptical centers which exhibit the same flatness. We deduce the neighborhood of each ellipse by using the same intersection method described in the section above, and use a simple bottom-up method to combine elliptical centers the elliptical error given by \\begin{equation} \\tilde{\\varepsilon} = \\frac{\\hat{\\lambda}_d}{\\sum_{j=1}^{d} \\hat{\\lambda}_j} \\end{equation} is less than some user specified tolerance \\(\\varepsilon\\).\nResults I have applied this method reconstruct the curve silhouette of the bunny model from sample points in 2D. As the compact RBF representation gradually transforms into an EBF representation, the contours sharpens - the best result probably is given in (c). However, note that as the ellipse thins, the internal and external contours deteriate, potentially leading to unpleasant numerical artefacts.\n            (a) (b) (c)             (d) (e) (f)   Gradually transforming the compact EBF shape matrices from radial (RBF) to elliptical. From (a) to (c), the sharpening of the resulting contour is clearly visible at the bunny foot. The shape contour begins to deteriorate in (d) to (e), as ellipses that are orthogonal to the surface begin influencing the interior of the shape. In this example, sigma is chosen to include the 10 nearest centers.   Conclusion In this technical report I have demonstrated a method to build and represent point set surfaces using a scattered data interpolation technique based on compactly supported elliptical basis functions (EBF\u0026rsquo;s). While the technique has been successfully employed elsewhere in representing volume (and image) data, it\u0026rsquo;s application to surfaces is largely unexplored.\nWhile this initial finding does show promise, my suspicion is that this approach has a number of considerable failings:\n Computation: It is computationally very expensive to solve the variational system for every elliptical basis function - which is the reason for no 3D results being included in this report. I believe that one possible option is to significantly improve the performance of the interpolation if only a limited subset of EBF\u0026rsquo;s are used to represent a shape, in the same way that a Gabor Wavelet filter bank has a limited number of filter orientations. In fact, an interesting idea for future work is to deduce an algorithm that adaptively determines the best orientations of a limited number of EBF\u0026rsquo;s in order to represent the shape. Accuracy: While some of the shapes in the results are promising, I am very concerned about the bottom row - as the EBF thins, the shape of the contour deteriorates significantly, which may cause numerical instabilities when the EBF\u0026rsquo;s are not chosen correctly. How to fit EBF\u0026rsquo;s to a surface without excessive thinning is a difficult problem, and certainly not addressed here.  Appendix: Minimum Volume Enclosing Ellipse Given \\(n\\) points \\(\\x_i,,i=1,\\ldots,n\\), find the minimum volume enclosing ellipsoid. This is effectively the optimization problem \\([\\min \\left[\\log\\left(\\det(Q)\\right)\\right],,\\mathrm{s.t.},,(\\x_i - \\q)^T Q (\\x_i - \\q) \\leq 1.\\)\nThis problem is solved using the Khachiyan method [11], also known as barycentric coordinate ascent. This approach finds the barycentric coordinates \\(u\\) of a center of the ellipse in terms of the input points \\(P\\) by an iterative algorithm which shifts \\(u\\) closer to the farthest point from the center \\(\\q=Pu\\). The optimal step-size \\(\\delta\\) is deduced using the method presented by Kachiyan [11]. In the algorithm, \\(\\e\\) is an \\(m\\)-length vector of ones and \\(\\e_j\\) is the \\(j_{th}\\) basis vector. This method is typically greatly accelerated by using only the points on the convex hull of \\(P\\). For this we use the QHull method [2].\nReferences [1] Salvatore Alfano and Meredith L. Greer. Determining if two solid ellipsoids intersect. Journal of Guidance, Control, and Dynamics, 26(1):106110, 2003.\n[2] C.B. Barber, D.P. Dobkin, and H.T. Huhdanpaa. The quickhull algorithm for convex hulls. ACM Trans. on Mathematical Software, 22(4):469483, December 1996. http://www.qhull.org.\n[3] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell,W. R. Fright, B. C. McCallum, and T. R. Evans. Reconstruction and representation of 3d objects with radial basis functions. In SIGGRAPH 01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 6776, New York, NY, USA, 2001. ACM. ISBN 1-58113-374-X.\n[4] G. Casciola, D. Lazzaro, L. B. Montefusco, and S. Morigi. Shape preserving surface reconstruction using locally anisotropic radial basis function interpolants. Comput. Math. Appl., 51(8):11851198, 2006. ISSN 0898-1221. doi: http://dx.doi.org/10.1016/j.camwa.2006.04.002.\n[5] G. Casciola, L. B. Montefusco, and S. Morigi. Edge-driven image interpolation using adaptive anisotropic radial basis functions. J. Math. Imaging Vis., 36(2):125139, 2010. ISSN 0924-9907. doi: http://dx.doi.org/10.1007/s10851-009-0176-8.\n[6] F. Chen and D. Suter. Multiple order laplacian spline  including splines with tension. Technical Report MECSE 19965, Dept. of Electrical and Computer Systems Engineering, Monash University, July 1996.\n[7] Huong Quynh Dinh and Greg Turk. Reconstructing surfaces using anisotropic basis functions. In International Conference on Computer Vision (ICCV) 2001, pages 606613, 2001.\n[8] Wei Hong, Neophytos Neophytou, Klaus Mueller, and Arie Kaufman. Constructing 3d elliptical gaussians for irregular data. In Mathematical Foundations of Scientific Visualization, Comp. Graphics, and Massive Data Exploration. Springer-Verlag, 2006.\n[9] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface reconstruction from unorganized points. In SIGGRAPH 92: Proceedings of the 19th annual conference on Computer graphics and interactive techniques, pages 7178, New York, NY, USA, 1992. ACM. ISBN 0-89791-479-1. doi: http://doi.acm.org/10.1145/133994.134011.\n[10] Y. Jang, R. P. Botchen, A. Lauser, D. S. Ebert, K. P. Gaither, and T. Ertl. Enhancing the interactive visualization of procedurally encoded multifield data with ellipsoidal basis functions. Computer Graphics Forum (Proceedings of Eurographics), 3(25), 2006.\n[11] Leonid G. Khachiyan. Rounding of polytopes in the real number model of computation. Mathematics of Operations Research, 21(2):307320, May 1996.\n[12] S. P. Lloyd. Least square quantization in PCM. IEEE Transactions on Information Theory, 28(2): 129137, 1982.\n[13] Boris Mederos Luiz, Luiz Velho, Luiz Henrique, and De Figueiredo. Moving least squares multiresolution surface approximation. In Brazilian Symposium on Computer Graphics and Image Processing SIBGRAPI, 2003.\n[14] Y. Ohtake, A. Belyaev, M. Alexa, G. Turk, and H. Seidel. Multi-level partition of unity implicits. In Proceedings of SIGGRAPH, pages 463470, 2003.\n[15] Y. Ohtake, A. Belyaev, and H. Seidel. 3d scattered data approximation with adaptive compactly supported radial basis functions. In Conf. Shape Modeling and Applications, pages 3139, 2004.\n[16] R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical Journal, 36(1):13891401, 1957.\n[17] G. Turk and J. F. OBrien. Variational implicit surfaces. Technical Report GIT- GVU-99-15, Georgia Institute of Technology, 1999.\n[18] H.Wendland. Piecewise polynomial, positive definite and compactly supported radial basis functions of minimal degree. In Advances in Computational Mathematics 4, pages 389396, 1995.\n","date":1264982400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1264982400,"objectID":"a605a3e09e62932f7c68ab29b94c6c54","permalink":"https://rsouthern.github.io/post/ebf/","publishdate":"2010-02-01T00:00:00Z","relpermalink":"/post/ebf/","section":"post","summary":"I present a method to reconstruct a surface representation from a a set of EBF's, and in addition present an efficient top-down method to build an EBF representation from a point cloud representation of a surface. I also discuss the advantages and disadvantages of this approach.","tags":["geometry processing","radial basis functions"],"title":"Compact Elliptical Basis Functions for Surface Reconstruction","type":"post"},{"authors":["lihua you","Richard Southern","jian j. zhang"],"categories":null,"content":"Media    ","date":1243814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1243814400,"objectID":"40f38cdb7aefbd53f744a2ca0e506857","permalink":"https://rsouthern.github.io/publication/physicsface/","publishdate":"2009-06-01T00:00:00Z","relpermalink":"/publication/physicsface/","section":"publication","summary":"A physics based model of for facial animation.","tags":["animation","facial animation","geometry processing"],"title":"Adaptive PhysicsInspired Facial Animation","type":"publication"},{"authors":["xiaosong yang","Richard Southern","jian j zhang"],"categories":null,"content":"Media       Fast Simulation of Skin Sliding  from Richard Southern   ","date":1243814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1243814400,"objectID":"a11bb0698a5b7ead444a5a12785798cf","permalink":"https://rsouthern.github.io/publication/skinslide/","publishdate":"2009-06-01T00:00:00Z","relpermalink":"/publication/skinslide/","section":"publication","summary":"We present a novel method to simulate the phenomenon of skin sliding in real-time by remeshing the surface based on a parameter space resampling.","tags":["geometry processing","simulation","parameterization"],"title":"Fast simulation of skin sliding","type":"publication"},{"authors":["richard southern","james gain"],"categories":null,"content":"","date":1046476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1046476800,"objectID":"00e6f51df4430418b6781697859dc26a","permalink":"https://rsouthern.github.io/publication/geomorph/","publishdate":"2003-03-01T00:00:00Z","relpermalink":"/publication/geomorph/","section":"publication","summary":"We present a new structure, the g-mesh, which greatly simplifies the implementation of continuous level of detail in large scenes.","tags":["rendering","cuda"],"title":"Creation and control of real-time continuous level of detail on programmable graphics hardware","type":"publication"},{"authors":["Richard Southern","simon perkins","barry steyn","alan muller","patrick marais","edwin blake"],"categories":null,"content":"Media    ","date":983404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":983404800,"objectID":"476ce368b883b095d1bcf142574a4212","permalink":"https://rsouthern.github.io/publication/viewdepend/","publishdate":"2001-03-01T00:00:00Z","relpermalink":"/publication/viewdepend/","section":"publication","summary":"We present a framework for real-time view-dependent refinement, and adapt it to the task of browsing large model repositories on the Internet.","tags":["geometry processing"],"title":"A Stateless Client for Progressive View-dependent Transmission","type":"publication"}]