<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>geometry processing | Richard's Blog</title><link>https://rsouthern.github.io/tag/geometry-processing/</link><atom:link href="https://rsouthern.github.io/tag/geometry-processing/index.xml" rel="self" type="application/rss+xml"/><description>geometry processing</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-uk</language><lastBuildDate>Fri, 01 Apr 2016 00:00:00 +0000</lastBuildDate><image><url>https://rsouthern.github.io/images/icon_hu0365f2187e15210a9118469a64220edc_142871_512x512_fill_lanczos_center_2.png</url><title>geometry processing</title><link>https://rsouthern.github.io/tag/geometry-processing/</link></image><item><title>n-Dimensional Moving Least Squares projection</title><link>https://rsouthern.github.io/post/mls/</link><pubDate>Fri, 01 Apr 2016 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/post/mls/</guid><description>&lt;h3 id="background">Background&lt;/h3>
&lt;p>Continuing with the theme of nD implementations of useful algorithms, here is my take on &lt;a href="https://en.wikipedia.org/wiki/Moving_least_squares">Moving Least Squares (MLS)&lt;/a>, a popular method used in engineering and graphics applications for smoothing out noisy point cloud data. Possibly data sources include laser scans, depth sensors (e.g. Kinect) or the like.&lt;/p>
&lt;p>The basic principle of MLS projection is that you have a &lt;em>noisy point cloud&lt;/em> approximating a &lt;em>surface&lt;/em>. Projection means we&amp;rsquo;re going to take a point from anywhere in space and project it on this surface approximation. We will conveniently also get a surface normal for free as a result of doing this.&lt;/p>
&lt;h3 id="the-basic-algorithm">The Basic Algorithm&lt;/h3>
&lt;p>I&amp;rsquo;ll describe the projection algorithm with reference to the following figures:&lt;/p>
&lt;table width=100%>
&lt;tr>
&lt;td>&lt;img width=100% src="mlsproject1.svg"/>&lt;/td>
&lt;td>&lt;img width=100% src="mlsproject2.svg"/>&lt;/td>
&lt;td>&lt;img width=100% src="mlsproject3.svg"/>&lt;/td>
&lt;/tr>
&lt;/table>
We are given the original query site \\(q_0)\in\mathbb{R}^n\\) (the red point) and the existing point cloud consisting of points \\(x_i \in \mathbb{R}^n\\) (the black points) (note I'm going to use the indices of the query site and the point cloud differently - the index of \\(q\\) refers to the iteration number of the algorith, the index of \\(x\\) will refer to index of the nearby points). We are also given some query radius \\(r\\) which will contain all the points of interest. Note that the choice of \\(r\\) has a significant effect on the smoothness of the function - this will probably be a topic of discussion for a later post. In the images from right to left:
1. Identify all the points within the query radius \\(r\\) of \\(q_0\\), i.e. the set of points \\(\\{x_i : \\|x_i - q_0\\| &lt; r\\}\\)
2. Construct a *best-fit plane* through these points by using a weighted least squares method (more on this later). Then *project* \\(q_0\\) onto this plane to find our next query point \\(q_1\\).
3. Iterate and continue until two consecutive steps of the projection result in very similar points, i.e. within some \\(\epsilon\\) which is some small error tolerance.
&lt;p>This is pretty trivially demonstrated using a code snippet from the implementation. In this example &lt;em>cpt&lt;/em> is the current point (e.g. the iterating projected point \(q_j\)), &lt;em>tol&lt;/em> is some user specified tolerance (equivalent to \(\epsilon\)). The rest should mostly be self-explanatory:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> // Continue this loop until we're close enough to the surface
while (err &amp;gt; tol) {
// Perform a least squares projection from the current point onto a best
// fit hyperplane. The result will tell us if any neighbours were found.
if (weightedLS(cpt, radius, plane, idxDist) == 0) {
// What should we do here? throw an error?
return cpt;
}
// If it all went well, we'll have a plane onto which the point can be projected
tmp = plane.projection(cpt);
// Our error is just the distance between the input point and our current point
err = (tmp-cpt).norm();
// Update our current point to the projected point
cpt = tmp;
}
// If it all went well, we can assume our current point is the best one available
return cpt;
&lt;/code>&lt;/pre>
&lt;p>Note that the function weightedLS() returns the number of points found within the query radius, and modifies the plane to contain the best fit plan through the points. If nothing was found within the radius there is a question of what you should do. Increasing the radius size is an obvious choice, but there may be continuity issues in the query data set.&lt;/p>
&lt;h3 id="weighted-least-squares">Weighted Least Squares&lt;/h3>
&lt;p>So it is now important to drill down into the Weighted Least Squares fit function. There are some pretty useful references on everyones favourite academic source &lt;a href="https://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares">here&lt;/a>. There is a much broader discussion here to be had about linear regression for fitting curves to polynomials, but I&amp;rsquo;ll restrict myself to the planar case.&lt;/p>
&lt;p>The general implicit form for a plane $$\mathbf{n}\cdot(x - x_0)=0,$$ where \(\mathbf{n}\) is a normal to the plane (bold because it is a vector), and \(x_0\) is a point through which the plane passes. So the plane is defined by any point \(x\) for which the above function evaluates to exactly \(0\).&lt;/p>
&lt;p>If the number of points is equal to the dimensionality the space it is easy enough to find the normal from the input points. For example, the normal to a vector
$$
\mathbf{v}= \left[ \begin{array}\ x_1 - x_0 \\ y_1 - y_0 \end{array} \right]
$$
between two points in \(\mathbb{R}^2\) is \((-v_y,v_x)\), for higher dimensions use the cross or wedge product.&lt;/p>
&lt;blockquote>
&lt;p>As an aside, you might be interested to know that this can also be solved by solving for the &lt;em>null space&lt;/em> of the system of equations given by \(\mathbf{n}\cdot(x - x_0)=0\) - recall that this system is underdetermined because one of the points provided will be \(x_0\). In \(\mathbb{R}^2\), this will be the equivalent of solving for the &lt;em>null space&lt;/em> of
$$
\begin{array}\&lt;br>
\mathbf{n} \cdot \mathbf{v} = 0\&lt;br>
n_x v_x + n_y v_y = 0
\end{array}
$$
which is of course only non-trivially true when \(\mathbf{n} = (-\mathbf{v}_y,\mathbf{v}_x)\).&lt;/p>
&lt;/blockquote>
&lt;p>Note that we can also solve for the normal using the formula \(\mathbf{n}\cdot(x - x_0)=\alpha\) where \(\alpha\) is some arbitrary scalar as the normal is the same if we shifted it along the normal direction. This allows us to formulate the problem into a linear system
$$X\textbf{n}=\textbf{b},$$
where \(X\) is the matrix of known points on the plane, \(\mathbf{n}\) is the unknown plane normal, and \(\mathbf{b}\) is some constant vector made up of \(\alpha\)&amp;lsquo;s. For simplicity let&amp;rsquo;s just let \(\alpha=1\) from now on. This is well defined if \(X\) is square, i.e. there are the same number of points as there are dimensions - this is because these points form a &lt;a href="https://en.wikipedia.org/wiki/Simplex">n-simplex&lt;/a> in \(\mathbb{R}^n\).&lt;/p>
&lt;p>Ok, now what happens if we have more points than dimensions? In this case, we can solve this in the &lt;em>least squares sense&lt;/em>. This is essentially an optimisation problem, solved using the Moore-Penrose &lt;a href="https://en.wikipedia.org/wiki/Least_squares">pseudoinverse&lt;/a>. While there is buckets to write about this, it boils down to premultiplying both sides of the equation by \(X^T\) yielding
$$X^T X \mathbf{n} = X^T \mathbf{b}.$$
Note that \(X^T X\) is now a square matrix, the system can be solved and has a unique solution. The solution itself is optimal in a least-squares sense, meaning that it &lt;em>minimizes the sum of the squares of the errors made in the results of each equation&lt;/em>. Or put another way, it spreads the error love evenly between all of the point contributors to the plane.&lt;/p>
&lt;p>Now applying weights to this should actually be quite straightforward: we&amp;rsquo;ll create some diagonal matrix of weights \(W\) where elements on the diagonal represent the amount we want to &lt;em>weight the error&lt;/em> of each point in \(X\). So a smaller weight would mean that in a least squares sense we are consider the error to be less important. This is incorporated into the system above as follows:
$$X^T W X \mathbf{n} = X^T W \mathbf{b}.$$
If we&amp;rsquo;re trying to write our plane equation as \(\mathbf{n}\cdot x = d\) where \(d\) is a constant, a property of the above process is that \(d=1 / |\mathbf{n}| \). Also you must still normalise the resultant normal, e.g. \(\mathbf{n} = \mathbf{n}d\). Note that this whole process works in any dimension - pretty neat.&lt;/p>
&lt;p>There is also the small issue of how to decide on the weights. Any kernel function would do, and each will have an impact on the overall quality of the reconstruction. Currently I&amp;rsquo;m just inverting the distance, but this could lead to problems when the point is exactly on top of the query point. Other choices of kernel function, such as \(\exp({-s^2})\). Some experimentation depending on application will probably be required.&lt;/p>
&lt;p>Here is the implementation of this in &lt;em>pointcloud.hpp&lt;/em>:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> if (m_index.findNeighbors(results, pt.data(), nanoflann::SearchParams())) {
// Weights matrix
MatrixXr W(idxDist.size(), idxDist.size()); W.setZero();
// The matrix of the actual points
MatrixXr K(idxDist.size(), DIM);
// The result of the call is true if a neighbour could be found
typename ResultsVector::iterator it;
int i,j;
for (it = idxDist.begin(), i=0; it != idxDist.end(); ++it,++i) {
// First check to see if this evaluates to 0 exactly (BAD)
if ((*it).second == REAL(0)) {
// Set the weight to half the max REAL value (risky)
W(i,i) = std::numeric_limits&amp;lt;REAL&amp;gt;::max() * REAL(0.5);
} else {
// Set the weight to the inverse distance
W(i,i) = 1.0 / (*it).second;
}
// Build the matrix K out of the point data
for (j=0;j&amp;lt;DIM;++j) K(i,j) = m_data[(*it).first][j];
}
// Normalise the weights to make sure they sum to 1
W = W / W.sum();
// Now solve the system using the following formula from MATLAB
// N' = (K'*diag(W)*K) \ (K')*diag(W)*(-ones(size(I,1),1));
// d = 1/norm(N);
// N = N' * d;
VectorDr ans;
MatrixXr A = K.transpose() * W * K;
VectorDr b = -K.transpose() * W.diagonal();
ans = A.llt().solve(b);
REAL d = REAL(1) / ans.norm();
Hyperplane _h(ans * d, d);
h = _h;
// For the results structure to be useful we need it to store the weights from W, so
// we copy these back into it
for (it = idxDist.begin(), i=0; it != idxDist.end(); ++it,++i) {
(*it).second = W(i,i);
}
// We return the number of effective matches
return idxDist.size();
} else {
// No neighbours found! What do we do?
return 0;
}
&lt;/code>&lt;/pre>
&lt;p>The general dimensional hyperplane is managed by &lt;a href="http://eigen.tuxfamily.org/dox/classEigen_1_1Hyperplane.html">Eigen::Hyperplane&lt;/a> which allows us to do projections (see previous code snippet). Note that the solver used is Eigen&amp;rsquo;s Cholesky solver, which is very fast by requires a matrix that is symmetric positive definite. Fortunately our matrix \(X^T W X\) has these properties. In practice it is very unwise to solve systems like \(A\textbf{x}=\textbf{b}\) by inverting \(A\).&lt;/p>
&lt;h3 id="implementation-details">Implementation details&lt;/h3>
&lt;p>The plan was to make this implementation general dimensional, and this has been achieved through liberal use of templates.&lt;/p>
&lt;pre>&lt;code class="language-cpp">template &amp;lt;typename REAL, unsigned int DIM&amp;gt;
class PointCloud {
&lt;/code>&lt;/pre>
&lt;p>The top of our class says that we&amp;rsquo;re allowing you to choose your type to represent the data (called REAL) and the dimension of the data set, which can be anything you like. As mentioned previously, Eigen provides general dimensional hyperplane routines which makes our lives relatively simple.&lt;/p>
&lt;p>Unfortunately as you may have realised from the above algorithm the performance is largely dependent on how quickly we can find all the points within a given radius. This is resolved using a &lt;a href="https://en.wikipedia.org/wiki/K-d_tree">KD tree&lt;/a>, which essentially organises the points into a graph structure for fast searching. I&amp;rsquo;m not going to discuss the specifics of KD tree construction and querying - there are plenty of other links to help you with this. I chose to make use of &lt;a href="https://github.com/jlblancoc/nanoflann">nanoflann&lt;/a> from Jose Luis Blanco-Claraco as it is a header only library making it easier to incorporate into the project build.&lt;/p>
&lt;p>One issue which may seem non-obvious is that I have added the KD tree index object as a member of the PointCloud class:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> /// A KDTreeIndex type
typedef nanoflann::KDTreeSingleIndexAdaptor&amp;lt;
nanoflann::L2_Simple_Adaptor&amp;lt;REAL, PointCloud&amp;lt;REAL,DIM&amp;gt; &amp;gt; ,
PointCloud&amp;lt;REAL,DIM&amp;gt;,
DIM&amp;gt; KDTreeIndex;
/// A KDTree structure
KDTreeIndex m_index;
&lt;/code>&lt;/pre>
&lt;p>This means that the KDTreeIndex is incorporated into the PointCloud class, although as you can see from the typedef above the PointCloud is a template parameter for the index itself. This is because the class which is passed to nanoflann needs to have a set of functions (all with the prefix kdtree_) which will be called during kd tree construction and querying, for example:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> /// Returns the dim'th component of the idx'th point in the class (random point access)
inline REAL kdtree_get_pt(const size_t idx, int dim) const {
return m_data[idx][dim];
}
&lt;/code>&lt;/pre>
&lt;p>The way this works is that we pass &lt;strong>this&lt;/strong> object when we construct the KDTreeIndex, e.g.&lt;/p>
&lt;pre>&lt;code class="language-cpp">/**
* Build an empty point cloud with an empty KDTree.
*/
template &amp;lt;typename REAL, unsigned int DIM&amp;gt;
PointCloud&amp;lt;REAL,DIM&amp;gt;::PointCloud()
: m_index(DIM, *this, nanoflann::KDTreeSingleIndexAdaptorParams(10)) {
}
&lt;/code>&lt;/pre>
&lt;p>It is an interesting and rather twisting compositional arrangement, but it allows us to make sure that all the elements relevent to the point cloud in one convenient place.&lt;/p>
&lt;p>Another interesting trick of nanoflann compatibility is the fact that I&amp;rsquo;m storing my points in Eigen, while nanoflann needs to compute distances using data stored in pointers to REAL&amp;rsquo;s. Rather than constructing a new Eigen point and copy data across, we can make use of the &lt;a href="http://eigen.tuxfamily.org/dox/classEigen_1_1Map.html">Eigen::Map&lt;/a> which will just make a vector out of the existing memory rather than copying it across, hopefully saving us a couple of clocks:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> /// Returns the distance between the vector &amp;quot;p1[0:size-1]&amp;quot; and the data point with index &amp;quot;idx_p2&amp;quot; stored in the class:
inline REAL kdtree_distance(const REAL *p1_data, const size_t idx_p2, size_t size) const {
Eigen::Map&amp;lt;const PointType&amp;gt; p1_map(p1_data,DIM);
return (m_data[idx_p2] - p1_map).norm();
}
&lt;/code>&lt;/pre>
&lt;h3 id="conclusions">Conclusions&lt;/h3>
&lt;p>Currently the examples of using this code are pretty spartan as it is going to be incorporate into a significantly larger project. However, there are a couple of important things to note about this implementation:&lt;/p>
&lt;ol>
&lt;li>The choice of radius has a significant impact on the projection. In previous implementations I&amp;rsquo;ve geometrically scaled the radius until some points are found and then scaled it back after the first projection. However, this may have continuity implications. I&amp;rsquo;ve also used a precomputed smooth scalar field of radii to use for this projection to ensure that you have a constant number of neighbours - something I&amp;rsquo;ll be experimenting with in the future.&lt;/li>
&lt;li>There are a couple of implementation issues I still need to resolve, such as the choice of weight function and how to handle the situation when there are no neighbours in the specificed radius (see above). There is also the relationship between the radius and the cell size of the KD Tree which needs to be resolved. And some nice 3D examples!&lt;/li>
&lt;li>The process of using MLS projection to smooth input data (called &lt;em>mollification&lt;/em>) was not discussed, but will hopefully be appended to this post at some point. The process is pretty simple: project each point in the existing set onto other points in the same set. This has some interesting performance considerations to doing it correctly (for example, the KD Tree will need to be rebuilt in each iteration).&lt;/li>
&lt;li>Parallelism! There are a couple of things which could be done in parallel, especially if we&amp;rsquo;re doing mollification.&lt;/li>
&lt;/ol>
&lt;h3 id="downloads">Downloads&lt;/h3>
&lt;p>The source code is hosted on &lt;a href="https://github.com/rsouthern/Examples">GitHub&lt;/a>. Clone it from the repository using the following command from your console:&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/rsouthern/Examples
&lt;/code>&lt;/pre></description></item><item><title>Recursive n-dimensional scalar field interpolation</title><link>https://rsouthern.github.io/post/scalarfield/</link><pubDate>Sat, 19 Sep 2015 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/post/scalarfield/</guid><description>&lt;h3 id="background">Background&lt;/h3>
&lt;p>Lets say you have a smooth scalar function of any number of variables \(f:\mathbb{R}^n\mapsto\mathbb{R}^1\). Let&amp;rsquo;s assume that the function is computationally expensive to evaluate, but you need to do this at run-time, possibly a lot. Let&amp;rsquo;s also assume that you can define the maximum and minimum range of values which you are interested in. Let&amp;rsquo;s also assume that the function is smooth, and you&amp;rsquo;re tolerant of (or even prefer) the results to be smoothed over.&lt;/p>
&lt;p>One possible solution is to sample values of the function over a regular grid, and perform some sort of interpolation between the values. So we&amp;rsquo;ll define a different function
\(g:\mathbb{R}^n\mapsto\mathbb{R}^1,,g\approx f\). Depending on the desired smoothness of the approximation different interpolation functions are feasible, but not all of these generalise to any dimension. One method which does, however, is the &lt;a href="https://en.wikipedia.org/wiki/Centripetal_Catmull%E2%80%93Rom_spline">Catmull-Rom Spline&lt;/a> (co-invented by the CG superhero &lt;a href="https://en.wikipedia.org/wiki/Edwin_Catmull">Ed Catmull&lt;/a>) which can be evaluated recursively on grid edges and exhibits \(C^2\) continuity.&lt;/p>
&lt;p>I&amp;rsquo;ll explain how this works with the help of this figure, representing a 2D scalar field:&lt;/p>
&lt;center>&lt;img src="catmullrom_interpolation.svg"/>&lt;/center>
In this case, we're looking at a function $f:\mathbb{R}^2\mapsto\mathbb{R}^1$ with the value returned by this function visualised as a height above the $(x,y)$ grid in the above image. We assume that the red points are the pre-computed values for $f$ at the corners of the grid cells. Once the cell in which the value to be calculated is identified, 4 Catmull-Rom interpolations are performed in the $y$ direction, to find the green points. One further Catmull-Rom interpolation is needed between these points to compute the final interpolated value (in blue). Note that the number of curves that need to interpolated in each dimension is multiplied by 4 in each step (so on a 3D grid, the first step requires 16 interpolations in the $z$, then 4 in the $y$ and 1 in the $x$). You can see that this process is ripe for recursion. It is also ripe for parallel computation, but that is another story...
&lt;h3 id="applications">Applications&lt;/h3>
&lt;p>You might be wondering when you might you want to use an application like this? I developed it in the context of a sparse, high dimensional &lt;a href="https://en.wikipedia.org/wiki/Moving_least_squares">Moving Least Squares&lt;/a> projection: if you are projecting onto a point cloud that is irregular, you need an adaptive, variable radius function in order to ensure that you have sufficient local points on which to project. As it&amp;rsquo;s expensive to compute local neighbours at run time (especially in, say, 66 dimensions) it&amp;rsquo;s probably a good idea to define an approximating smooth function which, given any point in space, returns a radius that will probably contain a desired number of point samples.&lt;/p>
&lt;h3 id="implementation">Implementation&lt;/h3>
&lt;p>There are a number of features used in this implementation, which I&amp;rsquo;m going to unpick from the implementation and discuss separately.&lt;/p>
&lt;h4 id="passing-around-functions">Passing around functions&lt;/h4>
&lt;p>Fundamental to this process is the ability to evaluate a generic function at a point. It would be very convenient for us to be able to pass said function around as a generic function pointer. For a simple generic function signature, I&amp;rsquo;ll use a function that accepts an array of &lt;em>double&lt;/em>s, and returns a &lt;em>double&lt;/em>. This can be stated using the &lt;a href="http://en.cppreference.com/w/cpp/utility/functional/function">std::function&lt;/a> method:&lt;/p>
&lt;pre>&lt;code class="language-cpp">std::function&amp;lt;double (double*)&amp;gt; _func;
&lt;/code>&lt;/pre>
&lt;p>Note that the type notation is C notation for a function that accepts a pointer to a double, and returns a double. This makes &lt;em>_func&lt;/em> a generic function that can be passed as a parameter - particularly convenient if we&amp;rsquo;re allowing the use of arbitrary input functions. Of course, the return type and array size could be specified as template parameters, but for interaction with existing functions (e.g. libnoise) I&amp;rsquo;ve kept it simple.&lt;/p>
&lt;h4 id="templates-and-c11">Templates and C++11&lt;/h4>
&lt;p>I do, however, use templates for the actual ScalarField class. In an early implementation of this structure I just used dynamic arrays of doubles to store the data - but this gives us the overhead of dynamic allocation and deallocation, along with the risks associated with non-contiguous memory. &lt;em>C++11&lt;/em> introduces the &lt;em>std::array&lt;/em>, which is essentially an old skool &lt;em>C&lt;/em> static array, but with all the handy goodness of the STL.&lt;/p>
&lt;p>This is from the top of the ScalarField class:&lt;/p>
&lt;pre>&lt;code class="language-cpp">template &amp;lt;uint DIM&amp;gt;
class ScalarField {
public:
/// Unsigned int array
typedef std::array&amp;lt;uint, DIM&amp;gt; uintArray;
/// Double array
typedef std::array&amp;lt;double, DIM&amp;gt; doubleArray;
&lt;/code>&lt;/pre>
&lt;p>The template parameter is evaluated at &lt;em>compile time&lt;/em>, generating a template class for the desired dimension. Whenever I create an object of type &lt;em>doubleArray&lt;/em> in the code, this is done statically.&lt;/p>
&lt;p>Note that I could have given a template parameter for a &lt;em>real type&lt;/em> and an &lt;em>integer type&lt;/em>, but that will add some unnecessary complexity for now.&lt;/p>
&lt;h4 id="recursive-algorithm">Recursive Algorithm&lt;/h4>
&lt;p>The core of the algorithm is the construction of the grid and the recursive evaluation of the query point. Fortunately these are reasonably similar. Lets have a look at the init routine:&lt;/p>
&lt;pre>&lt;code class="language-cpp">/**
* @brief ScalarField::initialise Inits the scalar field
* @param _func A boost-ified function declaration which returns the scalar value
* @param _minValue The minimum values of our given field.
* @param _maxValue The maximum values of our given field.
* @param _resolution The resolution of the field in each dimension.
*/
template&amp;lt;uint DIM&amp;gt;
void ScalarField&amp;lt;DIM&amp;gt;::init(std::function&amp;lt;double (double*)&amp;gt; _func,
const doubleArray &amp;amp;_minValue,
const doubleArray &amp;amp;_maxValue,
sconst uintArray&amp;amp; _resolution) {
// Check we've not been here before already
if (m_isInit) cleanup();
// Precompute the partition size in each case
// Now we can allocate some memory for our data
// Now for each value in the field we can use the provided function to derive the scalar value
// this is done recursively (could easily be done in parallel
doubleArray pos; uintArray idx;
initRecurse(_func, 0u, idx, pos);
// Set this to initialised
m_isInit = true;
}
&lt;/code>&lt;/pre>
&lt;p>Not very much excitement there: we set up the memory and ready ourselves for running the recursive function. The initRecurse function is obviously where the action is - so I&amp;rsquo;ll break it down into stages:&lt;/p>
&lt;pre>&lt;code class="language-cpp">/**
* @brief ScalarField::initRecurse Recursively initialise the scalar field along each dimension.
* @param _func The boostified function to execute to determine the scalar value
* @param dim The current dimension (start this at 0)
* @param idx The current index to evaluate (empty when you start)
* @param pos The position to evaluate (empty when you start)
*/
template&amp;lt;uint DIM&amp;gt;
void ScalarField&amp;lt;DIM&amp;gt;::initRecurse(std::function&amp;lt;double (double*)&amp;gt; _func,
const uint &amp;amp;dim,
uintArray &amp;amp;idx,
doubleArray &amp;amp;pos) {
&lt;/code>&lt;/pre>
&lt;p>The prototype reveals a lot about the functionality of the function. The first parameter is the function that we want to call at the grid points. The second parameter is the current dimension. You&amp;rsquo;ll notice from the code above that this starts at &lt;em>0&lt;/em> - each time the function is called, the dimension number will tick up until it gets to the actual dimension, which is our recursion termination criteria. Index and the position values fill up after the recursion unrolls.&lt;/p>
&lt;pre>&lt;code class="language-cpp"> // At the end of the recursion chain we can calculate the value using the function
if (dim &amp;gt;= DIM) {
// First determine the index in the data
...
// Now set the value of the data in the right place to the result of the function
m_data[dataIdx] = _func(&amp;amp;pos[0]);
return;
}
&lt;/code>&lt;/pre>
&lt;p>At the end of the recursion chain, we evaluate the given function at the position. This isn&amp;rsquo;t straightforward, as I&amp;rsquo;ve stored all the data in a big flat 1D array. However I&amp;rsquo;ve removed the details for simplicity.&lt;/p>
&lt;p>If the function didn&amp;rsquo;t return, we know that we&amp;rsquo;re not at the end of the chain so we recursively evaluate across each grid dimension:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> // If we didn't get to the end of the chain, we run another iteration
for (i = 0 ; i &amp;lt; m_resolution[dim]; ++i) {
idx[dim] = i;
pos[dim] = m_minValue[dim] + i * m_partSize[dim];
initRecurse(_func, dim+1, idx, pos);
}
&lt;/code>&lt;/pre>
&lt;p>In this way, each node in each dimension is initialised with our input function.&lt;/p>
&lt;p>The evaluation function is a bit more tricky:&lt;/p>
&lt;pre>&lt;code class="language-cpp">/**
* @brief ScalarField::eval Evaluate the scalar value at the specified point.
* @param pt The position at which to evaluate the scalar field
* @return
*/
template&amp;lt;uint DIM&amp;gt;
double ScalarField&amp;lt;DIM&amp;gt;::eval(const doubleArray&amp;amp; pt) const {
if (!m_isInit) return 0.0;
// Determine the index of the bottom corner of the evaluation grid (if outside, extrapolation
// may be used, so it's capped at the bounds of the grid)
...
// Deduce the index of the points from the min, max grid values and total resolution
...
// Evaluate the position recursively over all dimensions
double ret_val = evalRecurse(0, idx, currentIdx, x);
// Clear memory and return
return ret_val;
}
&lt;/code>&lt;/pre>
&lt;p>This is largely the same as the &lt;em>init&lt;/em> function above, except that you will need to calculate the bottom corner index of the grid cell in which the query point lives (which is messy, and hidden away).&lt;/p>
&lt;p>The header for &lt;em>evalRecurse&lt;/em> is more or less the same as &lt;em>initRecurse&lt;/em> so I won&amp;rsquo;t repeat myself. The big difference, of course, is that this function actually returns a value. Here&amp;rsquo;s the termination condition:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> // The termination condition - just return the scalar value at the specified position
if (dim &amp;gt;= DIM) {
// First determine the index in the data
...
// Just return the data value at this position!
return m_data[dataIdx];
}
&lt;/code>&lt;/pre>
&lt;p>This just returns the value at the grid point - this makes more sense in the context of the general condition below:&lt;/p>
&lt;pre>&lt;code class="language-cpp"> else {
// Evaluate the catmull rom spline recursively by combining the current indices
// along all dimensions
double v0, v1, v2, v3;
currentIdx[dim] = idx[dim] + 0; v0 = evalRecurse(dim+1, idx, currentIdx, x);
currentIdx[dim] = idx[dim] + 1; v1 = evalRecurse(dim+1, idx, currentIdx, x);
currentIdx[dim] = idx[dim] + 2; v2 = evalRecurse(dim+1, idx, currentIdx, x);
currentIdx[dim] = idx[dim] + 3; v3 = evalRecurse(dim+1, idx, currentIdx, x);
return catmullRomSpline(x[dim], v0, v1, v2, v3);
}
&lt;/code>&lt;/pre>
&lt;p>So this function spawns 4 different &lt;em>evalRecurse&lt;/em>s for this thread, each one potentially spawning another 4 &lt;em>ad nausium&lt;/em>. The results of each thread re interpolated using our magical &lt;em>catmullRomSpline&lt;/em> function (which is really easy - check the code).&lt;/p>
&lt;h3 id="conclusions">Conclusions&lt;/h3>
&lt;p>This is a nice example of an bit of code that is useful for teaching and research: there are a couple of nice &lt;em>C++11&lt;/em>, &lt;em>C++&lt;/em> and &lt;em>C&lt;/em> principles at play which greatly simplify the implementation and improve performance. The research behind it pretty nifty and broadly applicable (I would think).&lt;/p>
&lt;p>You may have observed that you could use this approach for general dimensional mapping, i.e. $f:\mathbb{R}^n\mapsto\mathbb{R}^m$, assuming that you create &lt;em>m&lt;/em> different ScalarFields. Vector Field intepolation might be more difficult as they will require some recombination function (e.g. normalisation), but not impossible.&lt;/p>
&lt;p>Parallelism is an obviously missing thing here: in very high dimensional data sets this could prove to be particularly useful - it is essentially a &lt;em>scan-reduce&lt;/em> function. Send me an email if you want this and I can check this out.&lt;/p>
&lt;h3 id="downloads">Downloads&lt;/h3>
&lt;p>The source code is hosted on &lt;a href="https://github.com/rsouthern/Examples">GitHub&lt;/a>. Clone it from the repository using the following command from your console:&lt;/p>
&lt;pre>&lt;code class="language-shell">git clone https://github.com/rsouthern/Examples
&lt;/code>&lt;/pre></description></item><item><title>Automatic cage construction for retargeted muscle fitting</title><link>https://rsouthern.github.io/publication/cagebased/</link><pubDate>Fri, 01 Mar 2013 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/publication/cagebased/</guid><description>&lt;h4 id="media">Media&lt;/h4>
&lt;center>
&lt;iframe width="480" height="320" src="https://www.youtube.com/embed/9uGBFbvoN_4" frameborder="0" allowfullscreen>&lt;/iframe>
&lt;/center>
&lt;pre>&lt;code>@article{
year={2013},
issn={0178-2789},
journal={The Visual Computer},
volume={29},
number={5},
doi={10.1007/s00371-012-0739-3},
title={Automatic cage construction for retargeted muscle fitting},
url={http://dx.doi.org/10.1007/s00371-012-0739-3},
publisher={Springer-Verlag},
keywords={Muscle modelling; Character animation},
author={Yang, Xiaosong and Chang, Jian and Southern, Richard and Zhang, JianJ.},
pages={369-380},
}
&lt;/code>&lt;/pre></description></item><item><title>Motion-Sensitive Anchor Identification of Least-Squares Meshes from Examples</title><link>https://rsouthern.github.io/publication/keypoints/</link><pubDate>Wed, 01 Jun 2011 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/publication/keypoints/</guid><description>&lt;h4 id="media">Media&lt;/h4>
&lt;center>
&lt;iframe width="480" height="320" src="https://www.youtube.com/embed/E_1n6eJ0Q4Q" frameborder="0" allowfullscreen>&lt;/iframe>
&lt;/center></description></item><item><title>The Force Density Method - A Brief Introduction</title><link>https://rsouthern.github.io/post/fdm/</link><pubDate>Tue, 01 Feb 2011 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/post/fdm/</guid><description>&lt;p>The method of force density was developed in response to the need for computational modelling of structures for the Munich Olympic complex [&lt;em>Lewis 2013&lt;/em>] . The method relies on the assumption that the ratio of tension force to length of each cable can be constant, transforming a system of non-linear equations to a set of linear equations which can be solved directly.&lt;/p>
&lt;p>The Force Density Method (FDM), first introduced by Scheck [&lt;em>1974&lt;/em>], is commonly used in engineering to find the equilibrium shape of a structure consisting of a network of cables with different elasticity properties when stress is applied. While shape analysis of tensile structures is a geometrically non-linear problem, the FDM linearises the form-fitting equations analytically by using the &lt;em>force density ratio&lt;/em> for each cable element, \(q = F/L\), where \(F\) and \(L\) are the force and length of a cable element respectively.&lt;/p>
&lt;h4 id="the-force-density-method">The Force Density Method&lt;/h4>
&lt;p>Given the positions of nodes (vertices) which connect the cables (edges) of our network \(V\), the topology of this network is encoded in the &lt;em>branch-node matrix&lt;/em> \(C\). The branch-node matrix of the network above is given by:
&lt;img src="cmatrix.svg" width=100%/>
Given a load vector \(R\) and the diagonal matrix of force densities \(Q\), the equilibrium location can be deduced by solving for \(X\) in&lt;/p>
&lt;p>$$(C^T Q C) X = R.$$&lt;/p>
&lt;h4 id="embedding">Embedding&lt;/h4>
&lt;p>In order to use the FDM for 2D embedding, we must be able to constrain nodes on the boundary. For this, the matrix \(C\) is separated into two sub&amp;ndash;matrices: \(C_f\) contains constrained nodes with corresponding position \(X_f\), while \(C\) contains those that are free to move. The problem is reformulated as
$$(C^T Q C) X = R - (C^T Q C_f)X_f.$$&lt;/p>
&lt;p>Note that for the purposes of embedding, \(R\) is typically set to zero.&lt;/p>
&lt;p>The FDM is &lt;em>fold-over free&lt;/em>. This is explained with reference to the following Figure:&lt;/p>
&lt;center>
&lt;img src="foldover.svg" width=40%/>
&lt;/center>
As the natural rest internal force load of each node is exactly 0, any foldover will result in external forces applied to the nodes which have folded over. As a result it will not be in a state of equilibrium, and this configuration cannot occur.
&lt;h4 id="stability-under-motion">Stability under motion&lt;/h4>
&lt;p>The FDM cannot guarantee any of the commonly advocated properties of embeddings in computer graphics, such as &lt;em>isometry&lt;/em> (length preserving) or &lt;em>conformality&lt;/em> (angle preserving). In our application, it is desirable that the path of a vertex or a group of vertices moving across the surface of the shape is mirrored in the embedding. We evaluate this phenomenon by measuring the distortion of these displacement vectors in the embedded space. We call this property &lt;em>stability under motion&lt;/em>.&lt;/p>
&lt;img src="graph.svg" width=100%/>
&lt;table>
&lt;tr>
&lt;td width=33%>&lt;img width=100% src="harmonic_embed.svg" />&lt;/td>
&lt;td width=33%>&lt;img width=100% src="mvc_embed.svg" />&lt;/td>
&lt;td width=33%>&lt;img width=100% src="fdm_embed.svg" />&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>In the above Figure we compare two popular fixed boundary conformal techniques, Harmonic mappings [&lt;em>Eck at al. 1995&lt;/em>] and Mean Value Coordinates [&lt;em>Floater 2003&lt;/em>] with an FDM network constructed with unit edge forces. The stability under motion of these embedding methods is highly non-linear, and so we evaluate each embedding technique experimentally as follows:&lt;/p>
&lt;ul>
&lt;li>Compute the embedding \(\mathcal{U}_0 = \mathsf{embed}\left(\mathcal{M}_0\right)\).&lt;/li>
&lt;li>Rotate a set of points on the sphere (in this case, one triangle) in a straight path around the surface of the sphere.&lt;/li>
&lt;li>For each state of the rotation \(\mathcal{M}_i\) compute \(\mathcal{U}_i=\mathsf{embed}\left(\mathcal{M}_i\right)\).&lt;/li>
&lt;li>Compute the \(n\) displacement vectors in the embedded space. For each \(u_{0,i} \in \mathcal{U}_O\) and \(u_{d,i} \in \mathcal{U}_d\) define \(\mathbf{d}_i\) as the vector between \(u_{0,i}\) and \(u_{d,i}\). Let \( D = \left[\mathbf{d}_1, \ldots , \mathbf{d}_n\right]^T \).&lt;/li>
&lt;li>We use the angle of the normal cone of these displacement vectors $$\alpha = \max_{i,j} \left(\mathrm{acos}(\mathbf{d}_i \cdot \mathbf{d}_j)\right)$$ as the distortion error. For this experiment, we evaluate the distortion of only the points moving on the surface.&lt;/li>
&lt;/ul>
&lt;p>We demonstrated that under all rotations the FDM embedding is stable, even when faces of \(\mathcal{M}_d\) overlap. In addition, there is considerably less displacement of surrounding nodes after rotations.&lt;/p>
&lt;h4 id="conclusion">Conclusion&lt;/h4>
&lt;p>I have presented the force density method as a technique to perform mesh embedding using this technique. It is computationally efficient, as it only involves the solution to a sparse linear system, easily solved with the Conjugate Gradient Method or using Cholesky Factorization. I demonstrated that embedding with FDM is very stable, preventing foldover and discontinuities when parameterizing an unstable triangulation. This is particularly useful when, for example, you need to flatten geometry in a stable fashion (for example, see Zhang et al. [2007] and our paper on skin sliding).&lt;/p>
&lt;h4 id="bibliography">Bibliography&lt;/h4>
&lt;p>Matthias Eck, Tony DeRose, Tom Duchamp, Hugues Hoppe, Michael Lounsbery, and Werner Stuetzle. Multiresolution analysis of arbitrary meshes. ACM Transactions on Graphics (Proceedings of SIGGRAPH &amp;lsquo;95). 173-182 &lt;a href="http://doi.acm.org/10.1145/218380.218440">doi: 10.1145/218380.218440&lt;/a>.&lt;/p>
&lt;p>Michael S. Floater, Mean value coordinates, Computer Aided Geometric Design, 20(1):19-27, 2003, ISSN 0167-8396, &lt;a href="http://dx.doi.org/10.1016/S0167-8396(03)00002-5">10.1016/S0167-8396(03)00002-5&lt;/a>.&lt;/p>
&lt;p>Wanda J. Lewis. Tension Structures: Form and Behaviour. Thomas Telford, illustrated edition, 2003. ISBN 0727732366, 9780727732361.&lt;/p>
&lt;p>J. H. Schek. The force density method for form finding and computation of general networks. Computer Methods in Applied Mechanics and Engineering, (3):115–134, 1974. &lt;a href="http://dx.doi.org/10.1016/0045-7825(74)90045-0">doi: 10.1016/0045-7825(74)90045-0&lt;/a>&lt;/p>
&lt;p>Zhang, J. J., Yang, X. and Zhao, Y. (2007), Bar-net driven skinning for character animation. Comp. Anim. Virtual Worlds, 18: 437–446. &lt;a href="http://onlinelibrary.wiley.com/doi/10.1002/cav.211/abstract">doi: 10.1002/cav.211&lt;/a>&lt;/p></description></item><item><title>Compact Elliptical Basis Functions for Surface Reconstruction</title><link>https://rsouthern.github.io/post/ebf/</link><pubDate>Mon, 01 Feb 2010 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/post/ebf/</guid><description>&lt;p>\(
\newcommand{\x}{\mathbf{x}}
\newcommand{\n}{\mathbf{n}}
\newcommand{\q}{\mathbf{q}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\diag}{\mathsf{diag}}
\newcommand{\khachiyan}{\mathsf{Khachiyan}}
\newcommand{\flatClust}{\mathsf{flatClust}}
\newcommand{\kmeans}{\mathit{k}\mathsf{means}}
\newcommand{\sort}{\mathsf{sort}}
\newcommand{\tP}{\tilde{P}}
\newcommand{\idx}{\mathbf{i}}
\newcommand{\tidx}{\tilde{\idx}}
\newcommand{\List}{\mathcal{L}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\eig}{\mathsf{eig}}
\)&lt;/p>
&lt;p>In this post I present a method to reconstruct a surface representation from a a set of EBF&amp;rsquo;s, and in addition present an efficient top-down method to build an EBF representation from a point cloud representation of a surface. I also discuss the advantages and disadvantages of this approach.&lt;/p>
&lt;p>Radial basis functions (RBFs) are a popular variational representation of volumes and surfaces in computer graphics. In general, an RBF is a real-valued function whose value depends only on the distance from it&amp;rsquo;s center. A special class of these functions have &lt;em>compact support&lt;/em> - in this case the function decays smoothly to zero as the radius approaches 1. In this way, only a relatively small number of RBF&amp;rsquo;s influence any particular point in space, which in turn greatly improves computational efficiency.&lt;/p>
&lt;table>
&lt;tr>
&lt;td>&lt;img width=50% src="ebf1x.png"/>&lt;/td>
&lt;td>&lt;img width=50% src="ebf2x.png"/>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;center>(a)&lt;/center>&lt;/td>
&lt;td>&lt;center>(b)&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>&lt;td colspan=2>&lt;it>On certain features (a), basic radial basis functions are inefficient at capturing the surface properties. In comparison (b), very few elliptical functions would be needed to represent this feature.&lt;/it>&lt;/td>&lt;/tr>
&lt;/table>
&lt;p>However, certain features are not best represented by radial basis functions, such as in Figure below. Consider two parallel lines - if they are close enough together, you will need many RBF&amp;rsquo;s in order to ensure that the two features are separated. In comparison, an &lt;em>elliptical&lt;/em> shape can better represent this structure (see figure).&lt;/p>
&lt;p>In this technical report I present a method to reconstruct a surface representation from a a set of EBF&amp;rsquo;s, and in addition present an efficient top&amp;ndash;down method to build an EBF representation from a point cloud representation of a surface. I also discuss the advantages and disadvantages of this approach.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The reader is probably familiar with the well known general elliptical form \(x^2 / a^2 + y^2 / b^2 = 1\). This 2D formulation assumes the ellipse is centered at the origin, \(a\) and \(b\) are the lengths of the major and minor axes which are aligned with the Cartesian axes. In general we use the quadratic form for an ellipse
\(
Ax^2 + By^2 + Cx + Dy + Exy + F = 1
\).
This can be rewritten in quadratic (matrix) form:&lt;/p>
&lt;p>\begin{equation}
f(\mathbf{x}) = (\mathbf{x}-\mathbf{q})^T Q (\mathbf{x}-\mathbf{q}) = 1 \label{eq:quadratic}
\end{equation}&lt;/p>
&lt;p>for ellipse center \(\mathbf{q}\) and shape matrix \(Q\). Note that for real roots, \(Q\) must be positive semi-definite, i.e. (\(Q = Q^T\) and \(\left\langle x, Qx \right\rangle \geq 0\) for all \(x\in\mathbb{R}^n\)). \(Q\) can be factorized into \(Q=M^TM\).&lt;/p>
&lt;p>An alternative, more compact formulation is to use homogeneous coordinates \(\hat{\mathbf{x}} = \left[ \mathbf{x}, 1\right]^T\) and combine \(\mathbf{q}\) and \(Q\) into a single matrix with a translational component:
\begin{equation}
A = \left[
\begin{array}{cc}
Q&amp;amp; 0\\&lt;br>
-\mathbf{q}&amp;amp; 1
\end{array}
\right]
\end{equation}&lt;/p>
&lt;p>so that quadratic form becomes the homogeneous form for the ellipsoid.
\begin{equation}
f(\hat{\x}) = \hat{\x}^T A \hat{\x} = 1 \label{eq:homogeneous}
\end{equation}
In general we refer to the ellipse by the pair \([\q, Q]\). Note that the quadratic and homogeneous forms of the ellipsoid compute the elliptical radius. Note that the volume of an ellipse is given by \(v = \sqrt{\det(Q)}\).&lt;p>&lt;/p>
&lt;h4 id="radial-basis-functions">Radial Basis Functions&lt;/h4>
&lt;p>Radial Basis Functions (RBF) provide a simple method to construct smooth implicit surfaces from data of arbitrary dimension. Given a matrix of sample points:
\(P = \left[\x_1, \dots,\x_n \right]\)
which we assume are generated by sampling on the smooth implicit surface
\(
\hat{f}(\mathbf{x}) = 0
\),
we estimate this function using the a standard RBF formulation
\begin{equation}
f(\x) = \sum_{\q \in \mathcal{C}} \alpha_i \phi_{\sigma_i} \left(\left\|\x - \q\right\|\right) + \mathbf{b}^T p(\x),
\label{eq:rbf}
\end{equation}
where \(\sigma\) is the local basis function radius for compactly supported RBFs \(\phi_{\sigma}(r) = \phi(r/\sigma)\), \(\phi(r)\) is a radial basis function, \(\mathbf{b}=[\beta_1,\ldots,\beta_{|p(\x)|}]^T\), \(\alpha_i\) and \(\beta_j\) are unknown coefficients and \(p(\x)\) is some polynomial in \(\x\) with \(|p(\x)|\) terms (A good choice for \(p(\x)\) is typically \(\x + 1\)). The set \(\mathcal{C}=\left\{\q_1,\ldots,\q_m\right\}\) contains the chosen (RBF) centers, and for a compact approximation we assume \(m \ll n\).&lt;/p>
&lt;p>The choice of the basis function \(\phi(r)\) depends on the application - we have used globally supported spline \(\phi(r)=r^2 \log(r)\), near compactly supported Gaussian \(\phi(r)=e^{-r^2}\) and compactly supported Wendland functions [18]
\begin{equation}
\phi(r)=(1-r)^4_{+}(4r+1)
\end{equation}
Note, the \(f(r)_{+}\) operator ensures positivity, i.e. if \(f(r) \lt 0\) then \(f(r)_{+} = 0\), else \(f(r)_{+} = f(r)\).&lt;/p>
&lt;p>Dinh and Turk [7] propose the use of the spline formulation of Chen and Suter [6] due to the ability to locally control the smoothness of the resulting surface. This formulation requires two additional smoothness parameters which must currently be chosen in an ad-hoc fashion. As we will define locally anisotropic basis functions, the derivation of locally adaptive variants of \(\phi\) adds an unnecessary layer of complexity that is best avoided.&lt;/p>
&lt;h4 id="variational-implicit-surface-approximation">Variational Implicit Surface Approximation&lt;/h4>
&lt;p>RBF&amp;rsquo;s have been used extensively for the interpolation of volumetric data, neural networks and smooth surface approximations [17,3]. For surface approximation, a subset of \(k\) input points are chosen as RBF centers are chosen from the input data \(\mathcal{C}_s\), and \(l\) additional centers are added which are known to be on the exterior of the object \(\mathcal{C}_e\), \(\mathcal{C}=\mathcal{C}_s \cup \mathcal{C}_e\).&lt;/p>
&lt;p>We use the fact that&lt;/p>
&lt;p>\begin{equation}
f(\q) = \left\{
\begin{array}{rl}
0,&amp; \q \in \mathcal{C}_s\\\\
-1,&amp; \q \in \mathcal{C}_e
\end{array}
\right.
\end{equation}&lt;/p>
&lt;p>in order to evaluate the coefficients \(\alpha_i\) using linear regression. The problem can be stated in matrix form&lt;/p>
&lt;p>\begin{equation}
\left[
\begin{array}{cccc}
\phi_{1,1}&amp;\cdots&amp;\phi_{1,m}&amp;p(\q_1)\\\\
\vdots&amp;\ddots&amp;\vdots&amp;\vdots\\\\
\phi_{m,1}&amp;\cdots&amp;\phi_{m,m}&amp;p(\q_m)\\\\
p(\q_1)^T&amp;\cdots&amp;p(\q_m)^T&amp;\mathbf{0}
\end{array}
\right]
\left[
\begin{array}{c}
\alpha_1\\\\
\vdots\\\\
\alpha_{k+l}\\\\
\mathbf{b}
\end{array}
\right]
=
\left[
\begin{array}{c}
f(\q_1)\\\\
\vdots\\\\
f(\q_{k+l})\\\\
\mathbf{0}
\end{array}
\right],
\end{equation}&lt;/p>
&lt;p>where \(\phi_{i,j} = \phi_{\sigma_i}(\left|\q_i - \q_j\right|)\). Using the linear system above we can solve for the coefficients \(\alpha_i\) and \(\mathbf{b}\), and using these the implicit surface can be evaluated at any point using the RBF formulation.&lt;/p>
&lt;p>Defining the locations of external centers \(\mathcal{C}_e\) requires some concept of the orientation of the surface. Often [14,15,17] an associated normal field is assumed. In these cases, an external center \(q_e\) is simply defined in terms of the center on the surface \(\q_s\), \(\q_e\) = \(\q_s\) + \(\psi \n_s\), for some \(\psi &amp;gt; 0\).&lt;/p>
&lt;h2 id="elliptical-basis-functions">Elliptical basis functions&lt;/h2>
&lt;center>
&lt;table width=40% align="right">
&lt;tr>
&lt;td width=40%>&lt;center>&lt;img width=100% src="warp.eps.svg"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;it>Anisotropic radial basis functions compute distances in the warped space, computed by applying the transformation matrix M.&lt;/it>&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/center>
&lt;p>The isotropic behavior of RBF interpolation and resulting smoothness is often not a desirable property. Consider the bunny&amp;rsquo;s ear in the figure. Because a single RBF center with a large \(\sigma_i\) is used to represent the flat part of the ear, the reconstruction does not reproduce this flat region. This problem could be solved by using many smaller centers to encode the flat region, but this can dramatically increase the size of the above linear system, making the problem expensive to solve.&lt;/p>
&lt;p>For flat oriented regions an &lt;em>ellipse&lt;/em> better approximates shape. Recall that the shape matrix ((Q\) describes the shape of the ellipse. In particular, because \(Q\) is positive semi-definite and symmetric, we can factorize it using singular value decomposition (in MATLAB, \([V, \lambda] = \eig(Q)\), \(M = V \diag(\sqrt{\lambda})V^T\)) to solve it \(Q=M^T M\). In this figure we &lt;em>warp&lt;/em> the input space by transforming the input points using the ellipse shape matrix, i.e. \(\x&amp;rsquo; = M(\x-\q)+\q\). This space warping procedure is a method for local anisotropic interpolation.&lt;/p>
&lt;h4 id="formulation">Formulation&lt;/h4>
&lt;p>For an elliptical basis function formulation we define our set of centers as&lt;/p>
&lt;p>\begin{equation}
\mathcal{C}=\left\{[\q_1,Q_1,\sigma_1], \ldots, [\q_m,Q_m,\sigma_m]\right\},
\end{equation}&lt;/p>
&lt;p>consisting of tuples containing the elliptical information. We can incorporate this local space warping matrix \(M\) in the RBF definition:
\begin{equation}
f_k(\x) = \sum_{\q \in \mathcal{C}} \alpha_{i,k} \phi_{\sigma_i} \left(\left|M_k (\x - \q)\right|\right) + \mathbf{b}_k^T p(\x).
\end{equation}
Note that we use a subscript \(k\) to denote which transformation function is used. The coefficients must now be computed for each EBF center (and hence each \(M_k\) using the linear system.&lt;/p>
&lt;p>The problem of locally anisotropic RBF&amp;rsquo;s is resolved using a &lt;em>partition of unity&lt;/em> approach. Loosely speaking, the coefficients \(\alpha_i\) and \(\beta_j\) are deduced for each of the elliptical centers \([\q,Q,\sigma]\in \mathcal{C}\). In order to evaluate an isovalue at some \(\x\), we compute a weight based on the proximity of \(\x\) from each center in the locally warped space. Then, the final isovalue is computed by computing the sum of these locally computed weighted functions.&lt;/p>
&lt;p>More formally, we compute the isovalue by defining a new combined isosurface function
\begin{equation}
g(\x) = \frac{\sum_{k=1}^{m} w_k(\x) f_k(\x)}{\sum_{k=1}^{m} w_k(\x)}
\label{eq:combined}
\end{equation}
with the isosurface at \(g(\x)=0\). By choosing a smooth weight function \(w_k(\x)\) we ensure that the reconstruction results of the combined isosurface function is also smooth. Casciola et al. [4] use the local weight function
\begin{equation}
w_k(\x) = \left(\frac{\left(\sigma_k - |M_k(\x - \q_k)|\right)_{+}}{\sigma_k |M_k(\x - \q_k)|}\right)^{\gamma_k},
\label{eq:weights}
\end{equation}
where \(\sigma_k\) is, the region of influence of each local anisotropic center and \(\gamma_k\) is a local regularization exponent. We have used \(\gamma_k=1\) for all our results. Note that \(\sigma_k\) here is used to both scale the radius in the EBF equation and to determine the weights, and is a measurement of the &lt;em>region of influence&lt;/em> of a compact elliptical basis function.&lt;/p>
&lt;p>So in summary, given a set of elliptical centers \(\mathcal{C}\) consisting of the position \(\q\), shape matrix \(Q\) and radius of influence \(\sigma\) of each center, we construct a variational implicit surface using elliptical basis functions as follows:&lt;/p>
&lt;ul>
&lt;li>For &lt;em>each center&lt;/em> \([\q_k,Q_k,\sigma_k]\in\mathcal{C}\), compute the coefficients \(\alpha_{i,k}\), \(\mathbf{b}_k\) using the linear system as a preprocess.&lt;/li>
&lt;li>For an input point \(\x\), compute each of the weights \(w_k\).&lt;/li>
&lt;li>Compute \(g(\x)\) using these weights in using the EBF formulation and the combined isosurface function.&lt;/li>
&lt;/ul>
&lt;h2 id="building-an-ebf-surface-from-point-data">Building an EBF surface from point data&lt;/h2>
&lt;p>In this section we focus on the construction of EBF surfaces from point cloud data in any dimension without any shape information, such as surface normals. In order to construct an EBF surface we need a number of components:&lt;/p>
&lt;ul>
&lt;li>The elliptical shape properties of each center \(\q_k\) and \(Q_k\),&lt;/li>
&lt;li>The radius of influence of each center \(\sigma_k\),&lt;/li>
&lt;li>A normal field for the determination of external centers \(\mathcal{C}_e\), and&lt;/li>
&lt;li>Some radial basis function \(\phi(r)\).&lt;/li>
&lt;/ul>
&lt;p>For our application, we choose the radius of influence arbitrarily as the minimum radius needed to enclose a user specified number of neighboring centers in the warped elliptical space. For the radial basis function \(\phi(r)\) we make use of one of the standard RBF functions, depending on the application. In the following sections we will present a method for geometrically identifying the EBF centers and the the local region of influence for each center, as well as discuss our method to deduce the location and orientation of the elliptical centers.&lt;/p>
&lt;h4 id="flatness-clustering">Flatness clustering&lt;/h4>
&lt;/p>Other authors have made use of either randomized [7] or bottom-up [4] approaches to selecting surface centers. Unfortunately these either yield unpredictable results, or are expensive because of the need to compute local curvature information at every input point.&lt;/p>
&lt;p>&lt;img src="alg1.png" alt="Algorithm for Flatness Clustering">&lt;/p>
&lt;center>
&lt;table width=30% align="left">
&lt;tr>
&lt;td width=30%>&lt;center>&lt;img width=100% src="bunny3d.png"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=30%>Constructing EBF's over a 3D point cloud.&lt;/td>
&lt;/tr>
&lt;/table>
&lt;/center>
&lt;p>We define a recursive top-down algorithm for partitioning an input set of points \(P\) into flat regions. Loosely speaking, we compute a minimum volume ellipse from the current list of points and measure the flatness. We measure the &amp;ldquo;flatness&amp;rdquo; by using the ratio of the minimum ellipse axis length over the sum of all elliptical axis lengths, similar to the the method of Luiz et al. [13]. If the surface is not sufficiently flat we subdivide the list of points by using a standard clustering algorithm, and append the results of recursive calls to the same function on each cluster.&lt;/p>
&lt;p>The \(\flatClust\) algorithm makes use of the Khachiyan method for finding the minimum volume ellipse \(\khachiyan(P, \varepsilon)\), further discussed in the appendix. The eigenanalysis function \(\eig\) returns both the eigenvectors \(V\) and eigenvalues \(\lambda\). \(\kmeans(P,\idx)\) uses the method of Lloyd [12] to cluster only the points in \(P\) with the indices \(\idx\), and returns the set \(\I = \left\{ \tidx_1, \ldots, \tidx_n \right\}\) with each \(\tidx_j\) containing the indices of \(P\) belonging to each of the \(n\) clusters. We have used \(n=2\) for best results, although convergence is often faster when using a larger number of clusters. This approach can easily be applied to 3D data, as in the figure above.&lt;/p>
&lt;h4 id="consistent-orientation">Consistent orientation&lt;/h4>
&lt;p>In order to determine the external elliptical centers \(\mathcal{C}_e\) we require a local surface normal \(\n_s\). We can easily deduce an &lt;em>unoriented&lt;/em> normal from the eigenvector of \(Q\) associated with it&amp;rsquo;s smallest eigenvalue.&lt;/p>
&lt;p>A popular method for orienting these normals is by using the propagation method of Hoppe et al. [9]. In brief, this method constructs a Riemannian graph by defining each normal (tangent plane) as the nodes and edges connecting them are deduced using some proximity metric (in [9] this is the distance between the centers). A cost associated with an edge connecting node \(N_i\) to \(N_j\) is defined as \(1-|\n_i\cdot \n_j|\). The tree is traversed with a minimal spanning tree [16]. Whenever an edge \((i,j)\) is traversed, the orientation of \(\n_i\) is corrected if \(\n_i \cdot \hat{\n}_j \lt 0\), where \(\hat{\n}\) has already been corrected.&lt;/p>
&lt;p>In order to approximate the Riemannian graph, and thereby reduce the computation time and errors arising from using a minimal spanning tree, we instead determine neighboring centers by using ellipse intersection. Traditional ellipse intersection techniques require computing the roots of a quadratic polynomial, which can be time-consuming to compute numerically.&lt;/p>
&lt;p>Alfano and Greer [1] present a method to test for the intersection of two ellipses \(A\) and \(B\) (in the homogeneous form). The roots of the intersection can be found by determining \([V,\lambda] = \eig(A^{-1} B)\) and testing eigenvectors associated with non-real or repeated eigenvalues. This approach is easy to implement and very efficient as \(A^{-1}\) can be precomputed for all ellipses.&lt;/p>
&lt;h4 id="consolidation">Consolidation&lt;/h4>
&lt;p>Because \(\kmeans\) clustering is not flatness sensitive, flat regions may become fragmented due to this procedure. An additional consolidation step is required to merge neighboring elliptical centers which exhibit the same flatness. We deduce the neighborhood of each ellipse by using the same intersection method described in the section above, and use a simple bottom-up method to combine elliptical centers the elliptical error given by
\begin{equation}
\tilde{\varepsilon} = \frac{\hat{\lambda}_d}{\sum_{j=1}^{d} \hat{\lambda}_j}
\end{equation}
is less than some user specified tolerance \(\varepsilon\).&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>I have applied this method reconstruct the curve silhouette of the bunny model from sample points in 2D. As the compact RBF representation gradually transforms into an EBF representation, the contours sharpens - the best result probably is given in (c). However, note that as the ellipse thins, the internal and external contours deteriate, potentially leading to unpleasant numerical artefacts.&lt;/p>
&lt;center>
&lt;table width=80%>
&lt;tr>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0001.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0002.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0003.eps.svg"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0001.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0002.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0003.eps.svg"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=33%>&lt;center>(a)&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>(b)&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>(c)&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0004.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0005.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="ebf_0006.eps.svg"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0004.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0005.eps.svg"/>&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>&lt;img width=100% src="contour_0006.eps.svg"/>&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td width=33%>&lt;center>(d)&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>(e)&lt;/center>&lt;/td>
&lt;td width=33%>&lt;center>(f)&lt;/center>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td colspan=3>&lt;it>Gradually transforming the compact EBF shape matrices from radial (RBF) to elliptical. From (a) to (c), the sharpening of the resulting contour is clearly visible at the bunny foot. The shape contour begins to deteriorate in (d) to (e), as ellipses that are orthogonal to the surface begin influencing the interior of the shape. In this example, sigma is chosen to include the 10 nearest centers.&lt;/it>&lt;/td>&lt;/tr>
&lt;/table>
&lt;/center>
&lt;h3 id="conclusion">Conclusion&lt;/h3>
&lt;p>In this technical report I have demonstrated a method to build and represent point set surfaces using a scattered data interpolation technique based on compactly supported elliptical basis functions (EBF&amp;rsquo;s). While the technique has been successfully employed elsewhere in representing volume (and image) data, it&amp;rsquo;s application to surfaces is largely unexplored.&lt;/p>
&lt;p>While this initial finding does show promise, my suspicion is that this approach has a number of considerable failings:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Computation:&lt;/strong> It is computationally very expensive to solve the variational system for every elliptical basis function - which is the reason for no 3D results being included in this report. I believe that one possible option is to significantly improve the performance of the interpolation if only a limited subset of EBF&amp;rsquo;s are used to represent a shape, in the same way that a Gabor Wavelet filter bank has a limited number of filter orientations. In fact, an interesting idea for future work is to deduce an algorithm that adaptively determines the best orientations of a limited number of EBF&amp;rsquo;s in order to represent the shape.&lt;/li>
&lt;li>&lt;strong>Accuracy:&lt;/strong> While some of the shapes in the results are promising, I am very concerned about the bottom row - as the EBF thins, the shape of the contour deteriorates significantly, which may cause numerical instabilities when the EBF&amp;rsquo;s are not chosen correctly. How to fit EBF&amp;rsquo;s to a surface without excessive thinning is a difficult problem, and certainly not addressed here.&lt;/li>
&lt;/ul>
&lt;h2 id="appendix-minimum-volume-enclosing-ellipse">Appendix: Minimum Volume Enclosing Ellipse&lt;/h2>
&lt;p>Given \(n\) points \(\x_i,,i=1,\ldots,n\), find the minimum volume enclosing ellipsoid. This is effectively the optimization problem
\([\min \left[\log\left(\det(Q)\right)\right],,\mathrm{s.t.},,(\x_i - \q)^T Q (\x_i - \q) \leq 1.\)&lt;/p>
&lt;p>&lt;img src="alg2.png" alt="Algorithm for Minimum Volume Enclosing Ellipse">&lt;/p>
&lt;p>This problem is solved using the Khachiyan method [11], also known as &lt;em>barycentric coordinate ascent&lt;/em>. This approach finds the barycentric coordinates \(u\) of a center of the ellipse in terms of the input points \(P\) by an iterative algorithm which shifts \(u\) closer to the farthest point from the center \(\q=Pu\). The optimal step-size \(\delta\) is deduced using the method presented by Kachiyan [11]. In the algorithm, \(\e\) is an \(m\)-length vector of ones and \(\e_j\) is the \(j_{th}\) basis vector. This method is typically greatly accelerated by using only the points on the convex hull of \(P\). For this we use the &lt;em>QHull&lt;/em> method [2].&lt;/p>
&lt;h3 id="references">References&lt;/h3>
&lt;p>[1] Salvatore Alfano and Meredith L. Greer. Determining if two solid ellipsoids intersect. Journal of Guidance, Control, and Dynamics, 26(1):106–110, 2003.&lt;/p>
&lt;p>[2] C.B. Barber, D.P. Dobkin, and H.T. Huhdanpaa. The quickhull algorithm for convex hulls. ACM Trans. on Mathematical Software, 22(4):469–483, December 1996. &lt;a href="http://www.qhull.org">http://www.qhull.org&lt;/a>.&lt;/p>
&lt;p>[3] J. C. Carr, R. K. Beatson, J. B. Cherrie, T. J. Mitchell,W. R. Fright, B. C. McCallum, and T. R. Evans. Reconstruction and representation of 3d objects with radial basis functions. In SIGGRAPH ’01: Proceedings of the 28th annual conference on Computer graphics and interactive techniques, pages 67–76, New York, NY, USA, 2001. ACM. ISBN 1-58113-374-X.&lt;/p>
&lt;p>[4] G. Casciola, D. Lazzaro, L. B. Montefusco, and S. Morigi. Shape preserving surface reconstruction using locally anisotropic radial basis function interpolants. Comput. Math. Appl., 51(8):1185–1198, 2006. ISSN 0898-1221. doi: &lt;a href="http://dx.doi.org/10.1016/j.camwa.2006.04.002">http://dx.doi.org/10.1016/j.camwa.2006.04.002&lt;/a>.&lt;/p>
&lt;p>[5] G. Casciola, L. B. Montefusco, and S. Morigi. Edge-driven image interpolation using adaptive anisotropic radial basis functions. J. Math. Imaging Vis., 36(2):125–139, 2010. ISSN 0924-9907. doi: &lt;a href="http://dx.doi.org/10.1007/s10851-009-0176-8">http://dx.doi.org/10.1007/s10851-009-0176-8&lt;/a>.&lt;/p>
&lt;p>[6] F. Chen and D. Suter. Multiple order laplacian spline — including splines with tension. Technical Report MECSE 1996–5, Dept. of Electrical and Computer Systems Engineering, Monash University, July 1996.&lt;/p>
&lt;p>[7] Huong Quynh Dinh and Greg Turk. Reconstructing surfaces using anisotropic basis functions. In International Conference on Computer Vision (ICCV) 2001, pages 606–613, 2001.&lt;/p>
&lt;p>[8] Wei Hong, Neophytos Neophytou, Klaus Mueller, and Arie Kaufman. Constructing 3d elliptical gaussians for irregular data. In Mathematical Foundations of Scientific Visualization, Comp. Graphics, and Massive Data Exploration. Springer-Verlag, 2006.&lt;/p>
&lt;p>[9] Hugues Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle. Surface reconstruction from unorganized points. In SIGGRAPH ’92: Proceedings of the 19th annual conference on Computer graphics and interactive techniques, pages 71–78, New York, NY, USA, 1992. ACM. ISBN 0-89791-479-1. doi: &lt;a href="http://doi.acm.org/10.1145/133994.134011">http://doi.acm.org/10.1145/133994.134011&lt;/a>.&lt;/p>
&lt;p>[10] Y. Jang, R. P. Botchen, A. Lauser, D. S. Ebert, K. P. Gaither, and T. Ertl. Enhancing the interactive visualization of procedurally encoded multi–field data with ellipsoidal basis functions. Computer Graphics Forum (Proceedings of Eurographics), 3(25), 2006.&lt;/p>
&lt;p>[11] Leonid G. Khachiyan. Rounding of polytopes in the real number model of computation. Mathematics of Operations Research, 21(2):307–320, May 1996.&lt;/p>
&lt;p>[12] S. P. Lloyd. Least square quantization in PCM. IEEE Transactions on Information Theory, 28(2): 129–137, 1982.&lt;/p>
&lt;p>[13] Boris Mederos Luiz, Luiz Velho, Luiz Henrique, and De Figueiredo. Moving least squares multiresolution surface approximation. In Brazilian Symposium on Computer Graphics and Image Processing SIBGRAPI, 2003.&lt;/p>
&lt;p>[14] Y. Ohtake, A. Belyaev, M. Alexa, G. Turk, and H. Seidel. Multi-level partition of unity implicits. In Proceedings of SIGGRAPH, pages 463–470, 2003.&lt;/p>
&lt;p>[15] Y. Ohtake, A. Belyaev, and H. Seidel. 3d scattered data approximation with adaptive compactly supported radial basis functions. In Conf. Shape Modeling and Applications, pages 31–39, 2004.&lt;/p>
&lt;p>[16] R. C. Prim. Shortest connection networks and some generalizations. Bell System Technical Journal, 36(1):1389–1401, 1957.&lt;/p>
&lt;p>[17] G. Turk and J. F. O’Brien. Variational implicit surfaces. Technical Report GIT- GVU-99-15, Georgia Institute of Technology, 1999.&lt;/p>
&lt;p>[18] H.Wendland. Piecewise polynomial, positive definite and compactly supported radial basis functions of minimal degree. In Advances in Computational Mathematics 4, pages 389–396, 1995.&lt;/p></description></item><item><title>Adaptive Physics–Inspired Facial Animation</title><link>https://rsouthern.github.io/publication/physicsface/</link><pubDate>Mon, 01 Jun 2009 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/publication/physicsface/</guid><description>&lt;h4 id="media">Media&lt;/h4>
&lt;center>
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/sWZJkIAif5N7zC" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen>&lt;/iframe>
&lt;/center></description></item><item><title>Fast simulation of skin sliding</title><link>https://rsouthern.github.io/publication/skinslide/</link><pubDate>Mon, 01 Jun 2009 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/publication/skinslide/</guid><description>&lt;h4 id="media">Media&lt;/h4>
&lt;center>
&lt;iframe src="https://player.vimeo.com/video/32458838" width="480" height="320" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen>&lt;/iframe>
&lt;/center>
&lt;center>
&lt;iframe src="//www.slideshare.net/slideshow/embed_code/key/LtMQQbEZazhpYS" width="595" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> &lt;/iframe> &lt;div style="margin-bottom:5px"> &lt;strong> &lt;a href="//www.slideshare.net/RichardSouthern2/fast-simulation-of-skin-sliding" title="Fast Simulation of Skin Sliding" target="_blank">Fast Simulation of Skin Sliding&lt;/a> &lt;/strong> from &lt;strong>&lt;a href="//www.slideshare.net/RichardSouthern2" target="_blank">Richard Southern&lt;/a>&lt;/strong> &lt;/div>
&lt;/center></description></item><item><title>A Stateless Client for Progressive View-dependent Transmission</title><link>https://rsouthern.github.io/publication/viewdepend/</link><pubDate>Thu, 01 Mar 2001 00:00:00 +0000</pubDate><guid>https://rsouthern.github.io/publication/viewdepend/</guid><description>&lt;h4 id="media">Media&lt;/h4>
&lt;center>
&lt;iframe width="480" height="320" src="https://www.youtube.com/embed/YfG1VgipFvM" frameborder="0" allowfullscreen>&lt;/iframe>
&lt;/center></description></item></channel></rss>